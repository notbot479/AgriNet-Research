{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85288427-7dc3-4053-a2c8-5f87930de968",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebf4c44-1f64-44bb-b5b3-d9af4cbb199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from os import PathLike\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b979f-719c-4023-8037-87a31234b556",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6482a40d-7491-4e9b-a967-b585bfff3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\n",
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021 C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(\"\")\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "KZ_BASE_DIR = os.path.join(ROOT_DIR, \"DJI_202507131523_004\")\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 512\n",
    "\n",
    "print(ROOT_DIR)\n",
    "print(BASE_DIR, KZ_BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf5685-ae57-445e-ad57-442d5171a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset_500 C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\\_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"_dataset_500\"\n",
    "kz_dataset_name = \"_dataset\"\n",
    "\n",
    "dataset_path = os.path.join(BASE_DIR, dataset_name)\n",
    "kz_dataset_path = os.path.join(KZ_BASE_DIR, kz_dataset_name)\n",
    "\n",
    "dataset_train_path = os.path.join(dataset_path, \"train\")\n",
    "dataset_val_path = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "dataset_test_path = os.path.join(dataset_path, \"test\")\n",
    "kz_dataset_test_path = os.path.join(kz_dataset_path, \"test\")\n",
    "\n",
    "# Train dataset paths\n",
    "IMAGE_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"images\", \"rgbn\"),\n",
    "    os.path.join(dataset_train_path, \"aug_images\", \"rgbn\"),\n",
    "]\n",
    "\n",
    "LABEL_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"labels\"),\n",
    "    os.path.join(dataset_train_path, \"aug_labels\"),\n",
    "]\n",
    "\n",
    "# Validation dataset paths\n",
    "# NOTE: Each label has a folder for each class (labels/class_x, etc.)\n",
    "VAL_IMAGE_RGB_DIR = os.path.join(dataset_val_path, \"images\", \"rgb\")\n",
    "VAL_IMAGE_NIR_DIR = os.path.join(dataset_val_path, \"images\", \"nir\")\n",
    "VAL_LABEL_DIRS = [\n",
    "    os.path.join(dataset_val_path, \"labels\"),\n",
    "]\n",
    "\n",
    "# Test dataset paths\n",
    "INCLUDE_US_TEST_DATASET = True\n",
    "INCLUDE_KZ_TEST_DATASET = False\n",
    "\n",
    "TEST_IMAGE_RGB_DIRS = []\n",
    "TEST_IMAGE_NIR_DIRS = []\n",
    "\n",
    "if INCLUDE_US_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "if INCLUDE_KZ_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "print(dataset_path, kz_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f0df-757f-46bf-be71-90ee09291786",
   "metadata": {},
   "source": [
    "# Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661ebf8c-23fa-4fd2-9ff3-b294102b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_from_numpy(arr: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(arr).long()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_image_from_path(cls, path: str | PathLike) -> torch.Tensor:\n",
    "        arr = np.array(PILImage.open(path))\n",
    "        return cls._load_image_from_numpy(arr)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class Image(ImageSource):\n",
    "    pass\n",
    "\n",
    "class Mask(ImageSource):\n",
    "    pass\n",
    "    \n",
    "@dataclass\n",
    "class ImageData:\n",
    "    image_id: str\n",
    "    image: Image\n",
    "    mask: Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb8927f-1b64-4756-a61e-807196ca7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBImagePlusNIR(Image):\n",
    "    \"\"\" RGB Image with an additional Near-Infrared (NIR) channel.\"\"\"\n",
    "\n",
    "    def __init__(self, rgb_path: str, nir_path: str, normalize=True):\n",
    "        self.rgb_path = rgb_path\n",
    "        self.nir_path = nir_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        rgb = self._load_image_from_path(self.rgb_path).float().permute(2, 0, 1)  # (3, H, W)\n",
    "        nir = self._load_image_from_path(self.nir_path).float().unsqueeze(0)      # (1, H, W)\n",
    "        img = torch.cat([rgb, nir], dim=0)  # (4, H, W)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class RGBNImage(Image):\n",
    "    \"\"\"Png image with 4 channels: Red, Green, Blue, Near-Infrared (NIR).\"\"\"\n",
    "\n",
    "    def __init__(self, rgbn_path: str, normalize=True):\n",
    "        if not self._is_png_image(rgbn_path):\n",
    "            raise ValueError(f\"RGBNImage only supports PNG images. Given file: {rgbn_path}\")\n",
    "        \n",
    "        self.rgbn_path = rgbn_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        img = self._load_image_from_path(self.rgbn_path).float().permute(2, 0, 1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_png_image(file_path: str) -> bool:\n",
    "        return file_path.lower().endswith('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b8735b-92ab-43eb-adda-be42a86d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotMask(Mask):\n",
    "    def __init__(self, *mask_paths: str, sort: bool = True):\n",
    "        self._mask_paths = sorted(mask_paths) if sort else mask_paths\n",
    "        self._labels_count = len(mask_paths)\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        tensors = []\n",
    "        \n",
    "        for mask_path in self._mask_paths:\n",
    "            image = self._load_image_from_path(mask_path)\n",
    "            mask = self._get_mask(image)\n",
    "            tensors.append(mask)\n",
    "\n",
    "        return torch.cat(tensors, dim=0)  # (C, H, W)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask(image: torch.Tensor) -> torch.Tensor:\n",
    "        return image.unsqueeze(0)  # (1, H, W)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        cls_name = self.__class__.__name__\n",
    "        labels_count = self._labels_count\n",
    "        return f\"{cls_name}(labels={labels_count})\"\n",
    "\n",
    "class EmptyMask(Mask):\n",
    "    def __init__(self, h: int, w: int, classes: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.classes = classes\n",
    "    \n",
    "    def load(self) -> torch.Tensor:\n",
    "        return torch.zeros(self.classes, self.h, self.w, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4532ce-8175-4302-b461-662c5a6ce719",
   "metadata": {},
   "source": [
    "# ImageIdsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9499d87e-9118-4013-88ee-8eda2112ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIdsParser:\n",
    "    @classmethod\n",
    "    def get_ids_for_dirs(cls, dirs: list[str]) -> list[str]:\n",
    "        ids = []\n",
    "        for d in dirs:\n",
    "            ids.extend(cls.get_ids_for_dir(d))\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ids_for_dir(cls, path: str) -> list[str]:\n",
    "        ids_with_nones = [cls._get_id_from_image_path(p) for p in cls._get_items_by_path(path)]\n",
    "        ids = [i for i in ids_with_nones if i]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_id_from_image_path(cls, path: str) -> str | None:\n",
    "        try:\n",
    "            return path.split(\".\")[0]  # id-with-coords.png\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_items_by_path(cls, path: str) -> list[str]:\n",
    "        try:\n",
    "            return os.listdir(path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] Path not found: {path}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cfb839-da01-4649-a26f-3af1c406fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 image IDs.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(IMAGE_DIRS)\n",
    "print(f\"Found {len(IMAGE_IDS)} image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c598e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 validation image IDs.\n"
     ]
    }
   ],
   "source": [
    "VAL_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs([VAL_IMAGE_RGB_DIR])\n",
    "print(f\"Found {len(VAL_IMAGE_IDS)} validation image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28bea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 test image IDs. Including KZ dataset: False\n"
     ]
    }
   ],
   "source": [
    "TEST_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(TEST_IMAGE_RGB_DIRS)\n",
    "print(f\"Found {len(TEST_IMAGE_IDS)} test image IDs. Including KZ dataset: {INCLUDE_KZ_TEST_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b5c3e-8bc6-4ea7-b25a-035a1b223061",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING IMAGE IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9165ec71-cb8b-4ecd-a829-70b577852a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Based on current IMAGE_IDS and _dataset_processed data fetch about 15000 instances foreach label, ignore all storm damage masks, create plot maybe\n",
    "# 2. Use pre-processed IMAGE_IDS below\n",
    "# 3. Image loader, background class 0 (value 0). Maybe hardcode background creation mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c545ca-8514-4f4f-87c7-cad1a7b42bcd",
   "metadata": {},
   "source": [
    "# Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b956e9db-0f4d-49e5-9e65-05dbfc1fb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(ABC, Dataset):\n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        self._image_ids = image_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self._image_ids[idx]\n",
    "        data = self.get_data(image_id)\n",
    "\n",
    "        img = data.image.load()\n",
    "        mask = data.mask.load()\n",
    "        \n",
    "        # Validate shapes match\n",
    "        if img.shape[-2:] != mask.shape[-2:]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch for {image_id}: \"\n",
    "                f\"image {img.shape} vs mask {mask.shape}\"\n",
    "            )\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def first(self) -> ImageData | None:\n",
    "        try:\n",
    "            first_id = self._image_ids[0]\n",
    "            return self.get_data(first_id)\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576e65-0a6a-4836-ad75-c40f3ed72e55",
   "metadata": {},
   "source": [
    "# File Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f762d464-bf74-4870-a63c-9aaeed5353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSearcher:\n",
    "    def __init__(self, search_paths: list[str]):\n",
    "        self.search_paths = search_paths\n",
    "        self._cache = self._build_index()\n",
    "    \n",
    "    def _build_index(self) -> dict[str, list[str]]:\n",
    "        \"\"\"Build filename -> paths mapping once\"\"\"\n",
    "        index = {}\n",
    "        for root_folder in self.search_paths:\n",
    "            for dirpath, _, filenames in os.walk(root_folder):\n",
    "                for filename in filenames:\n",
    "                    key = filename.lower()\n",
    "                    if key not in index:\n",
    "                        index[key] = []\n",
    "                    index[key].append(os.path.join(dirpath, filename))\n",
    "        return index\n",
    "    \n",
    "    def search(self, file_name: str) -> list[str]:\n",
    "        return self._cache.get(file_name.lower(), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea0521-ac26-4b73-a84d-7eba1a6831b3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05407576-cf2d-4add-9a0c-8bafd4f34008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 instances\n",
      "ImageData(image_id='11IE4DKTR_11556-9586-12068-10098', image=RGBNImage, mask=OneHotMask)\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = 9\n",
    "    \n",
    "    _image_searcher = FileSearcher(IMAGE_DIRS)\n",
    "    _mask_searcher = FileSearcher(LABEL_DIRS)\n",
    "    \n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        super().__init__(image_ids=image_ids)\n",
    "    \n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "    \n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        rgbn_paths = self._image_searcher.search(file_name)\n",
    "\n",
    "        if len(rgbn_paths) == 0 or len(rgbn_paths) > 1:\n",
    "            raise Exception(f\"Expected exactly one RGBN image for ID '{image_id}', found {len(rgbn_paths)}.\")\n",
    "\n",
    "        return RGBNImage(rgbn_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "\n",
    "        self._validate_masks(masks)\n",
    "\n",
    "        return OneHotMask(*masks)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(\"Failed parse masks, {expected=}, {found=}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def _test() -> None:\n",
    "    train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "    image_data = train_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} instances\")\n",
    "    \n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b5ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 instances in validation dataset\n",
      "ImageData(image_id='138GZZR3U_2368-12251-2880-12763', image=RGBImagePlusNIR, mask=OneHotMask)\n"
     ]
    }
   ],
   "source": [
    "class ValidationDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = 9\n",
    "\n",
    "    _rgb_searcher = FileSearcher([VAL_IMAGE_RGB_DIR])\n",
    "    _nir_searcher = FileSearcher([VAL_IMAGE_NIR_DIR])\n",
    "    _mask_searcher = FileSearcher(VAL_LABEL_DIRS)\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name)\n",
    "        nir_paths = self._nir_searcher.search(file_name)\n",
    "\n",
    "        if len(rgb_paths) != 1 or len(nir_paths) != 1:\n",
    "            raise Exception(\n",
    "                f\"Expected exactly one RGB and one NIR image for ID '{image_id}', \"\n",
    "                f\"found rgb={len(rgb_paths)}, nir={len(nir_paths)}.\"\n",
    "            )\n",
    "\n",
    "        return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_mask_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}' in validation set.\")\n",
    "\n",
    "        self._validate_masks(masks)\n",
    "\n",
    "        return OneHotMask(*masks)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, expected={expected}, found={found}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.jpg\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "def _test() -> None:\n",
    "    val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "    image_data = val_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(val_dataset)} instances in validation dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ce87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 instances in test dataset\n",
      "ImageData(image_id='17FCUVCMI_1728-7254-2240-7766', image=RGBImagePlusNIR, mask=EmptyMask)\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(SegmentationDataset):\n",
    "    _rgb_searcher = FileSearcher(TEST_IMAGE_RGB_DIRS)\n",
    "    _nir_searcher = FileSearcher(TEST_IMAGE_NIR_DIRS)\n",
    "    _dummy_mask_width = _dummy_mask_height = 512\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        image = self._get_image(image_id)\n",
    "        mask = EmptyMask(\n",
    "            h=self._dummy_mask_height,\n",
    "            w=self._dummy_mask_width,\n",
    "            classes=9,\n",
    "        )\n",
    "\n",
    "        return ImageData(image_id=image_id, image=image, mask=mask)\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name_png = f\"{image_id}.png\"\n",
    "        file_name_jpg = f\"{image_id}.jpg\"\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name_jpg) + self._rgb_searcher.search(file_name_png)\n",
    "        nir_paths = self._nir_searcher.search(file_name_jpg) + self._nir_searcher.search(file_name_png)\n",
    "\n",
    "        if len(rgb_paths) == 1 and len(nir_paths) == 1:\n",
    "            return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "        \n",
    "        raise Exception(f\"Failed to resolve test image for ID '{image_id}': rgb={len(rgb_paths)}, nir={len(nir_paths)}\")\n",
    "\n",
    "def _test() -> None:\n",
    "    test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "    image_data = test_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(test_dataset)} instances in test dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef1f41-5b1c-4f15-9e9e-e6718576200c",
   "metadata": {},
   "source": [
    "# Build model with SegmentationModels.Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b96e7e-1462-4225-8ca7-40da70ab1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.metrics import iou_score, get_stats\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd2aee77-3ed6-4102-b879-0f102daa4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGBN_CHANNELS = 4\n",
    "CLASSES_COUNT = 9\n",
    "\n",
    "SMP_MODEL_NAME = \"resnet50\"\n",
    "\n",
    "def get_model():\n",
    "    return smp.DeepLabV3Plus(\n",
    "    encoder_name=SMP_MODEL_NAME,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=RGBN_CHANNELS,\n",
    "    classes=CLASSES_COUNT,\n",
    ")\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbe565-c5f2-4cb7-84e3-8bf4b9850385",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4867cc26-80a9-4753-95dd-df0ad654a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_type() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57827020-2595-47f3-aed4-18666065d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected batch size: 8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 8 if get_device_type() == \"cuda\" else 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "print(f\"Selected batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de372f-82a4-41e0-8a49-4e2287254c3c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a61900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # Using multilabel Dice loss since targets are one-hot encoded per class (B, C, H, W)\n",
    "    return dice_loss(pred, target)\n",
    "\n",
    "dice_loss = smp.losses.DiceLoss(mode='multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9511591-fde0-4caf-a69a-f64daa653f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs: torch.Tensor, masks: torch.Tensor, num_classes=9):\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    if masks.dim() == 4:\n",
    "        target_idx = torch.argmax(masks, dim=1)\n",
    "    else:\n",
    "        target_idx = masks\n",
    "    \n",
    "    tp, fp, fn, tn = get_stats(\n",
    "        preds.long(), \n",
    "        target_idx.long(), \n",
    "        mode='multiclass', \n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Use built-in functions\n",
    "    from segmentation_models_pytorch.metrics import f1_score\n",
    "    iou = iou_score(tp, fp, fn, tn, reduction='micro')\n",
    "    dice = f1_score(tp, fp, fn, tn, reduction='micro')  # F1 = Dice\n",
    "    \n",
    "    return dice.item(), iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "437c9d2f-4bee-46ec-bcb7-edac04d2e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler=None\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgs, masks in loader:\n",
    "        # imgs: (B, 4, H, W), masks: (B, C, H, W) one-hot\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with autocast(device_type):\n",
    "                outputs = model(imgs)\n",
    "                loss = calculate_loss(outputs, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate_one_epoch(model: torch.nn.Module, loader: DataLoader, num_classes=9) -> tuple[float, float, float]:\n",
    "    \"\"\"Validate the model for one epoch. Returns loss, dice, iou.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            \n",
    "            dice, iou = calculate_metrics(outputs, masks, num_classes=num_classes)\n",
    "            total_dice += dice * imgs.size(0)\n",
    "            total_iou += iou * imgs.size(0)\n",
    "    \n",
    "    dataset_size = len(loader.dataset)\n",
    "    return (total_loss / dataset_size,\n",
    "            total_dice / dataset_size,\n",
    "            total_iou / dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba7cba6-6324-4cb8-b14e-775818523aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device_type = get_device_type()\n",
    "device = torch.device(device_type)\n",
    "print(f\"Using device: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac26037",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler(device_type) if device_type == 'cuda' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a843868a-a9f1-4d8e-8862-381cb1c5f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use mIoU metric and calc metric foreach class (background + target classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8015ff3a-34d6-4944-bab6-128720e9d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 0.4877 | Val Loss: 0.1318 | Dice: 0.0869 | IoU: 0.0517\n",
      "✓ Saved new best model (IoU: 0.0517)\n",
      "Epoch [2/20] | Train Loss: 0.4734 | Val Loss: 0.1317 | Dice: 0.0754 | IoU: 0.0453\n",
      "Epoch [3/20] | Train Loss: 0.4526 | Val Loss: 0.1316 | Dice: 0.0766 | IoU: 0.0464\n",
      "Epoch [4/20] | Train Loss: 0.4265 | Val Loss: 0.1317 | Dice: 0.0765 | IoU: 0.0463\n",
      "Epoch [5/20] | Train Loss: 0.4254 | Val Loss: 0.1317 | Dice: 0.0812 | IoU: 0.0487\n",
      "Epoch [6/20] | Train Loss: 0.4153 | Val Loss: 0.1318 | Dice: 0.0751 | IoU: 0.0455\n",
      "Early stopping triggered after 6 epochs\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "MODEL_NAME = f\"best_model_{SMP_MODEL_NAME}.pth\"\n",
    "\n",
    "def train_model() -> None:\n",
    "    best_val_iou = 0.0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "        val_loss, val_dice, val_iou = validate_one_epoch(model, val_loader, num_classes=9)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "              f\"| Train Loss: {train_loss:.4f} \"\n",
    "              f\"| Val Loss: {val_loss:.4f} \"\n",
    "              f\"| Dice: {val_dice:.4f} \"\n",
    "              f\"| IoU: {val_iou:.4f}\")\n",
    "        \n",
    "        # Save best model only\n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            patience_counter = 0\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_iou': val_iou,\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_NAME)\n",
    "            print(f\"✓ Saved new best model (IoU: {val_iou:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30172e13-665b-4749-8ddb-ae869e5fb226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to eval mode on cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "# Robust checkpoint loading: supports raw state_dict or full checkpoint\n",
    "ckpt = torch.load(MODEL_NAME, map_location=device, weights_only=True)\n",
    "\n",
    "if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "    state_dict = ckpt[\"model_state_dict\"]\n",
    "else:\n",
    "    # Assume it's a plain state_dict\n",
    "    state_dict = ckpt\n",
    "\n",
    "# Strip potential 'module.' prefixes (if saved from DataParallel)\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace(\"module.\", \"\")\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "if missing:\n",
    "    print(f\"Warning: missing keys when loading: {missing}\")\n",
    "if unexpected:\n",
    "    print(f\"Warning: unexpected keys when loading: {unexpected}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded and set to eval mode on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5298930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img0 shape: torch.Size([4, 512, 512])\n",
      "gt0 (one-hot) shape: torch.Size([9, 512, 512])\n",
      "pred0 class-index mask shape: torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch from the validation DataLoader\n",
    "imgs, gt_masks = next(iter(test_loader))  # imgs: (B, 4, H, W), gt_masks: (B, C, H, W)\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "# Run the loaded model to get predicted logits\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs)  # (B, 9, H, W)\n",
    "    probs = torch.softmax(logits, dim=1)  # (B, 9, H, W)\n",
    "    pred_masks_idx = torch.argmax(probs, dim=1)  # (B, H, W)\n",
    "\n",
    "# Select the first image/mask from the batch\n",
    "img0 = imgs[0]  # (4, H, W)\n",
    "gt0 = gt_masks[0]  # (9, H, W) one-hot\n",
    "pred0_idx = pred_masks_idx[0]  # (H, W)\n",
    "\n",
    "print(f\"img0 shape: {img0.shape}\")\n",
    "print(f\"gt0 (one-hot) shape: {gt0.shape}\")\n",
    "print(f\"pred0 class-index mask shape: {pred0_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c787f7",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "palette = [\n",
    "    (255, 0, 0),      # class 0 - red\n",
    "    (0, 255, 0),      # class 1 - green\n",
    "    (0, 0, 255),      # class 2 - blue\n",
    "    (255, 255, 0),    # class 3 - yellow\n",
    "    (255, 0, 255),    # class 4 - magenta\n",
    "    (0, 255, 255),    # class 5 - cyan\n",
    "    (255, 165, 0),    # class 6 - orange\n",
    "    (128, 0, 128),    # class 7 - purple\n",
    "    (128, 128, 128),  # class 8 - gray\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "702bb29c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def predict_mask(data: ImageData) -> torch.Tensor:\n",
    "    img = data.image.load()\n",
    "    img_batch = img.unsqueeze(0).to(device)  # (1, 4, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_batch)          # (1, 9, H, W)\n",
    "        probs = torch.softmax(logits, dim=1)  # (1, 9, H, W)\n",
    "        pred_idx = torch.argmax(probs, dim=1) # (1, H, W)\n",
    "\n",
    "    return pred_idx[0]  # (H, W)\n",
    "\n",
    "def display_mask(image: torch.Tensor, palette: list[tuple[int, int, int]]):\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "\n",
    "    color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    mask_np = image.cpu().numpy()\n",
    "\n",
    "    for c, color in enumerate(palette):\n",
    "        color_img[mask_np == c] = color\n",
    "\n",
    "    display(PILImage.fromarray(color_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac4e9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class-index mask shape: torch.Size([512, 512])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAIAAgADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDWooor80PyAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDD1DxF9hvpLb7Lv2Y+bzMZyAemPektvFFrKcXETwknqPmUD+f6VneKPI/tFPL/wBds/eYxj2/HH6YrDr1qWFpTpptan75kfA2RZpk1CvOjKE5xV2pSve1m0m3HV6rS1mjs5vEmnR42NJLnrsTGPzxVH/hLf8Apy/8i/8A1q5qitI4Kit1c9bC+GvD9GPLUpyqPvKTT/8AJeVfh0OmXxYpYbrMhc8kSZIH0xVj/hKbH/nlcf8AfK/41yNFDwVF9C63hvw9UacaTj6Sl+rZ13/CU2P/ADyuP++V/wAahl8VxBh5Nq7rjq7hTn6DNcvRQsFRXQml4a8PwleVOUvJyf6Wf4nTL4sUsN1mQueSJMkD6Yroo5FliSRDlHUMp9Qa83r0Oyg+y2UMGFBRADt6E9z+dcmMo06aXLofAeIvDeU5PRoTwMOSU201dtNJLX3m3o2tu5PRRRXAflYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFRXE6WtvJPIcIi5Pv7fWpa5nxTeKxislwSp8xz6HoB+p/StqFP2k1E97hnJZZzmdPBr4XrJ9orf/ACXm0YV5dNeXktw4wXbOPQdh+VQUUV7ySSsj+raFGnQpRo0laMUkl2S0S+4KKKKZqFFFFABRRTo42llSNBl3YKo9SaRMpRhFyk7JF/RbBr7UEyB5URDvuXIIz0/H/Gu5qnp2nQ6bb+XHyx5dyOWP+HtVyvExNb2s7rZH8w8a8SLPcx9pS/hQ0hfT1fzf4W63CiiiuY+PCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBk00dvC0srhI1GSxrgdQuvtt/NcYwHbgY7DgfjgV0nii5MVjHbgkGZsnjggf/XI/KuSr1cDStH2j6n7x4W5HCjg5ZrP46l4rtypq/zck/uXmFFFFegfrAUUUUAFFFFABXQeFYEe4nnYZeNQF9s5yfrx+prn67HwxGyaUWYYDyll9xgD+YNcuMly0X5nw3iLjHhsgqqLs5uMd7bu79bpNW7XNmiiivEP5oCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5fxXbsJ4Lnkqy+WeOAQc9ffJ/KudrtfEUavo0rMMlGVl9jkD+RNcVXtYKfNS9D+k/DfHvF5DCEt6UnD8pL8JJfIKKKK6z70llt5oUjeSNlWVdyEj7w/z/SoqsXF7PdQwRSsCsC7UwMcf5AH4VXqY3t7xzYR4h0r4lJTu/hva13y76/Da/ncKKKKo6Qr0DTFVNLtAqgDylOAO5GTXn9elV52YPSKPx7xcruNDC0f5nJ/cor/ANu/q4UUUV5Z+IBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZviD/kCXH/Af/QhXD132qQfadLuYsMSUJAXqSOQPzFcDXrYB/u2vM/ffCetB5VWpL4lUu/Rxil+TCiiiu8/UwooooAKKKKAJ7KNZb+3jcZR5VVh6gmvQ643w5ZLdagZZFJSABhg4+bPH8ifwrsq8nHzTmo9j8B8VcfTrZlSwsHd0469k5Wf32tfysFFFFcB+WhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFebMrIxVlKspwQRgg16TXBavE0OrXSsQSZC3HoeR/OvRy+XvSR+veEeJUcVisP1lGMv/AW1/wC3FKiiivUP3IKKVVZzhVJOCcAdhyaSgV1ewUUVasNPn1CcRxKdoI3v2UE9f/rVMpKKuzHFYqjhKMq+Ikowjq29kdZ4ftjbaTGWBDSkyEE+vT9AK1KRVVFCqoVVGAAMAClr5+c+eTk+p/IuaY6eYY2ri57zk36Xe3yWgUUUVBwBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFYfiWw8+zF0i5kh+9gclf/AK3X863KK0p1HTmpI9PJs1q5TjqeNo7we3ddV81oea0V0uq+HGaUzWCrg8tFnGD/ALPbHt/+qsz/AIR/VP8An1/8iL/jXtQxNKSvex/S2X8Y5LjcPGusRCF94zkoyT7NN/itH0ZDp2oPptw00aK5ZCmG/wA+oFVppnuJmlkILscsQoGT9BXQ2Hhhtwe+cbcAiONuc+hP+H51qroOmIwYWoyDnl2I/ImsZYujCTa1Z81juPeHMBjp1qUXUqtJOcEmrLZXcl87JrveyOb0rRJtQxK58u33YLd29dv+P86660tIbG3WCBcKOpPVj6n3qeivPrYidV67dj8l4l4ux+fVLVXy0k9ILZebfV+f3JBRRRXOfKhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_data = test_dataset.first()\n",
    "\n",
    "pred_idx_mask = predict_mask(test_image_data)\n",
    "print(f\"Predicted class-index mask shape: {pred_idx_mask.shape}\")\n",
    "\n",
    "display_mask(pred_idx_mask, palette=palette)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
