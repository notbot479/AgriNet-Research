{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85288427-7dc3-4053-a2c8-5f87930de968",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebf4c44-1f64-44bb-b5b3-d9af4cbb199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from os import PathLike\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b979f-719c-4023-8037-87a31234b556",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6482a40d-7491-4e9b-a967-b585bfff3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\n",
      "c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021 c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\n",
      "Classes count: 8\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(\"\")\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "KZ_BASE_DIR = os.path.join(ROOT_DIR, \"DJI_202507131523_004\")\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "RESUME_TRAINING = True\n",
    "USE_PRECOMPUTED_CLASS_WEIGHTS = True\n",
    "\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 512\n",
    "RGBN_CHANNELS = 4\n",
    "\n",
    "LABELS = [\n",
    "    \"double_plant\",\n",
    "    \"drydown\",\n",
    "    \"endrow\",\n",
    "    \"nutrient_deficiency\",\n",
    "    \"waterway\",\n",
    "    \"water\",\n",
    "    \"planter_skip\",\n",
    "    \"weed_cluster\",\n",
    "    \"storm_damage\",\n",
    "]\n",
    "\n",
    "EXCLUDE_LABELS = [\"storm_damage\"]\n",
    "CLASSES_COUNT = len(LABELS) - len(EXCLUDE_LABELS)\n",
    "\n",
    "print(ROOT_DIR)\n",
    "print(BASE_DIR, KZ_BASE_DIR)\n",
    "print(f\"Classes count: {CLASSES_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf5685-ae57-445e-ad57-442d5171a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\\_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"_dataset\"\n",
    "kz_dataset_name = \"_dataset\"\n",
    "\n",
    "dataset_path = os.path.join(BASE_DIR, dataset_name)\n",
    "kz_dataset_path = os.path.join(KZ_BASE_DIR, kz_dataset_name)\n",
    "\n",
    "dataset_train_path = os.path.join(dataset_path, \"train\")\n",
    "dataset_val_path = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "dataset_test_path = os.path.join(dataset_path, \"test\")\n",
    "kz_dataset_test_path = os.path.join(kz_dataset_path, \"test\")\n",
    "\n",
    "# Train dataset paths\n",
    "IMAGE_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"images\", \"rgbn\"),\n",
    "    os.path.join(dataset_train_path, \"aug_images\", \"rgbn\"),\n",
    "]\n",
    "\n",
    "LABEL_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"labels\"),\n",
    "    os.path.join(dataset_train_path, \"aug_labels\"),\n",
    "]\n",
    "\n",
    "# Validation dataset paths\n",
    "# NOTE: Each label has a folder for each class (labels/class_x, etc.)\n",
    "VAL_IMAGE_RGB_DIR = os.path.join(dataset_val_path, \"images\", \"rgb\")\n",
    "VAL_IMAGE_NIR_DIR = os.path.join(dataset_val_path, \"images\", \"nir\")\n",
    "VAL_LABEL_DIRS = [\n",
    "    os.path.join(dataset_val_path, \"labels\"),\n",
    "]\n",
    "\n",
    "# Test dataset paths\n",
    "INCLUDE_US_TEST_DATASET = True\n",
    "INCLUDE_KZ_TEST_DATASET = False\n",
    "\n",
    "TEST_IMAGE_RGB_DIRS = []\n",
    "TEST_IMAGE_NIR_DIRS = []\n",
    "\n",
    "if INCLUDE_US_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "if INCLUDE_KZ_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "print(dataset_path, kz_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f0df-757f-46bf-be71-90ee09291786",
   "metadata": {},
   "source": [
    "# Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661ebf8c-23fa-4fd2-9ff3-b294102b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_from_numpy(arr: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(arr.astype(np.float32))\n",
    "\n",
    "    @classmethod\n",
    "    def _load_image_from_path(cls, path: str | PathLike) -> torch.Tensor:\n",
    "        arr = np.array(PILImage.open(path))\n",
    "        return cls._load_image_from_numpy(arr)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class Image(ImageSource):\n",
    "    pass\n",
    "\n",
    "class Mask(ImageSource):\n",
    "    pass\n",
    "    \n",
    "@dataclass\n",
    "class ImageData:\n",
    "    image_id: str\n",
    "    image: Image\n",
    "    mask: Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb8927f-1b64-4756-a61e-807196ca7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBImagePlusNIR(Image):\n",
    "    \"\"\" RGB Image with an additional Near-Infrared (NIR) channel.\"\"\"\n",
    "\n",
    "    def __init__(self, rgb_path: str, nir_path: str, normalize=True):\n",
    "        self.rgb_path = rgb_path\n",
    "        self.nir_path = nir_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        rgb = self._load_image_from_path(self.rgb_path).permute(2, 0, 1)  # (3, H, W)\n",
    "        nir = self._load_image_from_path(self.nir_path).unsqueeze(0)      # (1, H, W)\n",
    "        img = torch.cat([rgb, nir], dim=0)  # (4, H, W)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class RGBNImage(Image):\n",
    "    \"\"\"Png image with 4 channels: Red, Green, Blue, Near-Infrared (NIR).\"\"\"\n",
    "\n",
    "    def __init__(self, rgbn_path: str, normalize=True):\n",
    "        if not self._is_png_image(rgbn_path):\n",
    "            raise ValueError(f\"RGBNImage only supports PNG images. Given file: {rgbn_path}\")\n",
    "        \n",
    "        self.rgbn_path = rgbn_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        img = self._load_image_from_path(self.rgbn_path).permute(2, 0, 1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_png_image(file_path: str) -> bool:\n",
    "        return file_path.lower().endswith('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b8735b-92ab-43eb-adda-be42a86d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMask(Mask):\n",
    "    \"\"\"Multi-label mask where labels CAN overlap (no mutual exclusivity).\"\"\"\n",
    "    \n",
    "    def __init__(self, *mask_paths: str, labels_order: list[str] | None = None):\n",
    "        paths = list(mask_paths)\n",
    "        if labels_order:\n",
    "            self._mask_paths = self._sort_by_labels(paths, labels_order)\n",
    "        else:\n",
    "            self._mask_paths = sorted(paths)\n",
    "        self._labels_count = len(self._mask_paths)\n",
    "\n",
    "    def get_label_order(self) -> list[str]:\n",
    "        return [self._get_label_from_path(p) for p in self._mask_paths]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_label_from_path(path: str) -> str:\n",
    "        return os.path.basename(os.path.dirname(path)).lower()\n",
    "\n",
    "    @classmethod\n",
    "    def _sort_by_labels(cls, mask_paths: list[str], labels_order: list[str]) -> list[str]:\n",
    "        path_by_label = {}\n",
    "        for path in mask_paths:\n",
    "            label = cls._get_label_from_path(path)\n",
    "            path_by_label[label] = path\n",
    "\n",
    "        sorted_paths = []\n",
    "        for label in labels_order:\n",
    "            label_lower = label.lower()\n",
    "            if label_lower in path_by_label:\n",
    "                sorted_paths.append(path_by_label[label_lower])\n",
    "        return sorted_paths\n",
    "\n",
    "    def load(self) -> torch.Tensor:\n",
    "        tensors = []\n",
    "        for mask_path in self._mask_paths:\n",
    "            image = self._load_image_from_path(mask_path)\n",
    "            mask = (image > 0).float().unsqueeze(0)  # (1, H, W)\n",
    "            tensors.append(mask)\n",
    "        \n",
    "        return torch.cat(tensors, dim=0)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(classes={self._labels_count})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bfb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyMask(Mask):\n",
    "    def __init__(self, h: int, w: int, classes: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.classes = classes\n",
    "\n",
    "    def load(self) -> torch.Tensor:\n",
    "        return torch.zeros(self.classes, self.h, self.w, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4532ce-8175-4302-b461-662c5a6ce719",
   "metadata": {},
   "source": [
    "# ImageIdsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9499d87e-9118-4013-88ee-8eda2112ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIdsParser:\n",
    "    @classmethod\n",
    "    def get_ids_for_dirs(cls, dirs: list[str]) -> list[str]:\n",
    "        ids = []\n",
    "        for d in dirs:\n",
    "            ids.extend(cls.get_ids_for_dir(d))\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ids_for_dir(cls, path: str) -> list[str]:\n",
    "        ids_with_nones = [cls._get_id_from_image_path(p) for p in cls._get_items_by_path(path)]\n",
    "        ids = [i for i in ids_with_nones if i]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_id_from_image_path(cls, path: str) -> str | None:\n",
    "        try:\n",
    "            return path.split(\".\")[0]  # id-with-coords.png\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_items_by_path(cls, path: str) -> list[str]:\n",
    "        try:\n",
    "            return os.listdir(path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] Path not found: {path}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cfb839-da01-4649-a26f-3af1c406fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 156397 image IDs.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(IMAGE_DIRS)\n",
    "print(f\"Found {len(IMAGE_IDS)} image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c598e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18334 validation image IDs.\n"
     ]
    }
   ],
   "source": [
    "VAL_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs([VAL_IMAGE_RGB_DIR])\n",
    "print(f\"Found {len(VAL_IMAGE_IDS)} validation image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28bea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19708 test image IDs. Including KZ dataset: False\n"
     ]
    }
   ],
   "source": [
    "TEST_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(TEST_IMAGE_RGB_DIRS)\n",
    "print(f\"Found {len(TEST_IMAGE_IDS)} test image IDs. Including KZ dataset: {INCLUDE_KZ_TEST_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c545ca-8514-4f4f-87c7-cad1a7b42bcd",
   "metadata": {},
   "source": [
    "# Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b956e9db-0f4d-49e5-9e65-05dbfc1fb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(ABC, Dataset):\n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        self._image_ids = image_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self._image_ids[idx]\n",
    "        data = self.get_data(image_id)\n",
    "\n",
    "        img = data.image.load()\n",
    "        mask = data.mask.load()\n",
    "        \n",
    "        # Validate shapes match\n",
    "        if img.shape[-2:] != mask.shape[-2:]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch for {image_id}: \"\n",
    "                f\"image {img.shape} vs mask {mask.shape}\"\n",
    "            )\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def first(self) -> ImageData | None:\n",
    "        try:\n",
    "            first_id = self._image_ids[0]\n",
    "            return self.get_data(first_id)\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576e65-0a6a-4836-ad75-c40f3ed72e55",
   "metadata": {},
   "source": [
    "# File Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f762d464-bf74-4870-a63c-9aaeed5353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSearcher:\n",
    "    def __init__(self, search_paths: list[str], exclude_patterns: list[str] | None = None):\n",
    "        self.search_paths = search_paths\n",
    "        self.exclude_patterns = exclude_patterns or []\n",
    "        self._cache = self._build_index()\n",
    "    \n",
    "    def _build_index(self) -> dict[str, list[str]]:\n",
    "        \"\"\"Build filename -> paths mapping once, excluding specified patterns\"\"\"\n",
    "        index = {}\n",
    "        for root_folder in self.search_paths:\n",
    "            for dirpath, _, filenames in os.walk(root_folder):\n",
    "                \n",
    "                # Check if current dirpath should be excluded\n",
    "                if self._should_exclude(dirpath):\n",
    "                    continue\n",
    "                    \n",
    "                for filename in filenames:\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    \n",
    "                    key = filename.lower()\n",
    "                    if key not in index:\n",
    "                        index[key] = []\n",
    "                    index[key].append(full_path)\n",
    "        return index\n",
    "    \n",
    "    def _should_exclude(self, path: str) -> bool:\n",
    "        \"\"\"Check if path contains any exclude patterns\"\"\"\n",
    "        path_lower = path.lower()\n",
    "        should_exclude = any(pattern.lower() in path_lower for pattern in self.exclude_patterns)\n",
    "\n",
    "        if should_exclude:\n",
    "            print(f\"Exclude path from search: {path}\")\n",
    "        \n",
    "        return should_exclude\n",
    "    \n",
    "    def search(self, file_name: str) -> list[str]:\n",
    "        return self._cache.get(file_name.lower(), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea0521-ac26-4b73-a84d-7eba1a6831b3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05407576-cf2d-4add-9a0c-8bafd4f34008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude path from search: c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset\\train\\labels\\storm_damage\n",
      "Exclude path from search: c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset\\train\\aug_labels\\storm_damage\n",
      "Loaded 156397 instances\n",
      "ImageData(image_id='11IE4DKTR_11556-9586-12068-10098', image=RGBNImage, mask=MultiLabelMask)\n",
      "Labels order in mask tensor\n",
      "['double_plant', 'drydown', 'endrow', 'nutrient_deficiency', 'waterway', 'water', 'planter_skip', 'weed_cluster']\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT\n",
    "    \n",
    "    _image_searcher = FileSearcher(IMAGE_DIRS)\n",
    "    _mask_searcher = FileSearcher(\n",
    "        LABEL_DIRS, \n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "    \n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        super().__init__(image_ids=image_ids)\n",
    "    \n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "    \n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        rgbn_paths = self._image_searcher.search(file_name)\n",
    "\n",
    "        if len(rgbn_paths) == 0 or len(rgbn_paths) > 1:\n",
    "            raise Exception(f\"Expected exactly one RGBN image for ID '{image_id}', found {len(rgbn_paths)}.\")\n",
    "\n",
    "        return RGBNImage(rgbn_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "        self._validate_masks(masks)\n",
    "        return MultiLabelMask(*masks, labels_order=LABELS)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, {expected=}, {found=}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def _test() -> None:\n",
    "    train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "    image_data = train_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} instances\")\n",
    "    \n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "        print(\"Labels order in mask tensor\")\n",
    "        print(image_data.mask.get_label_order())\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b5ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude path from search: c:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset\\val\\labels\\storm_damage\n",
      "Loaded 18334 instances in validation dataset\n",
      "ImageData(image_id='138GZZR3U_2368-12251-2880-12763', image=RGBImagePlusNIR, mask=MultiLabelMask)\n"
     ]
    }
   ],
   "source": [
    "class ValidationDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT\n",
    "\n",
    "    _rgb_searcher = FileSearcher([VAL_IMAGE_RGB_DIR])\n",
    "    _nir_searcher = FileSearcher([VAL_IMAGE_NIR_DIR])\n",
    "    _mask_searcher = FileSearcher(\n",
    "        VAL_LABEL_DIRS,\n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name)\n",
    "        nir_paths = self._nir_searcher.search(file_name)\n",
    "\n",
    "        if len(rgb_paths) != 1 or len(nir_paths) != 1:\n",
    "            raise Exception(\n",
    "                f\"Expected exactly one RGB and one NIR image for ID '{image_id}', \"\n",
    "                f\"found rgb={len(rgb_paths)}, nir={len(nir_paths)}.\"\n",
    "            )\n",
    "\n",
    "        return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_mask_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "        self._validate_masks(masks)\n",
    "        return MultiLabelMask(*masks, labels_order=LABELS)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, expected={expected}, found={found}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.jpg\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "def _test() -> None:\n",
    "    val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "    image_data = val_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(val_dataset)} instances in validation dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ce87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19708 instances in test dataset\n",
      "ImageData(image_id='17FCUVCMI_1728-7254-2240-7766', image=RGBImagePlusNIR, mask=EmptyMask)\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(SegmentationDataset):\n",
    "    _rgb_searcher = FileSearcher(TEST_IMAGE_RGB_DIRS)\n",
    "    _nir_searcher = FileSearcher(TEST_IMAGE_NIR_DIRS)\n",
    "    _dummy_mask_width = IMAGE_WIDTH\n",
    "    _dummy_mask_height = IMAGE_HEIGHT\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        image = self._get_image(image_id)\n",
    "        mask = EmptyMask(\n",
    "            h=self._dummy_mask_height,\n",
    "            w=self._dummy_mask_width,\n",
    "            classes=CLASSES_COUNT,\n",
    "        )\n",
    "\n",
    "        return ImageData(image_id=image_id, image=image, mask=mask)\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name_png = f\"{image_id}.png\"\n",
    "        file_name_jpg = f\"{image_id}.jpg\"\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name_jpg) + self._rgb_searcher.search(file_name_png)\n",
    "        nir_paths = self._nir_searcher.search(file_name_jpg) + self._nir_searcher.search(file_name_png)\n",
    "\n",
    "        if len(rgb_paths) == 1 and len(nir_paths) == 1:\n",
    "            return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "        \n",
    "        raise Exception(f\"Failed to resolve test image for ID '{image_id}': rgb={len(rgb_paths)}, nir={len(nir_paths)}\")\n",
    "\n",
    "def _test() -> None:\n",
    "    test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "    image_data = test_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(test_dataset)} instances in test dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de372f-82a4-41e0-8a49-4e2287254c3c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d26c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b361ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = min(0, os.cpu_count() or 1)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1283d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_MODEL = \"efficientnet-b3\"\n",
    "\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 8 if DEVICE.type == \"cuda\" else 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \n",
    "    # Model\n",
    "    \"encoder\": BACKBONE_MODEL,\n",
    "    \"in_channels\": 4,  # RGBN\n",
    "    \"classes\": CLASSES_COUNT,\n",
    "    \n",
    "    # Loss\n",
    "    \"bce_weight\": 0.5,\n",
    "    \"dice_weight\": 0.5,\n",
    "    \n",
    "    # Early stopping\n",
    "    \"patience\": 10,\n",
    "    \n",
    "    # Threshold for predictions\n",
    "    \"threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec2e1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined BCE + Dice Loss for multi-label segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        \n",
    "        # Store class weights for manual weighting\n",
    "        if class_weights is not None:\n",
    "            if not isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "            # Ensure it's 1D and detached\n",
    "            self.register_buffer('class_weights', class_weights.detach().clone().view(-1))\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Use BCE without pos_weight, we'll apply weights manually\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def dice_loss(self, pred, target, smooth=1.0):\n",
    "        \"\"\"Dice loss for multi-label.\"\"\"\n",
    "        pred = torch.sigmoid(pred)\n",
    "\n",
    "        # Flatten per class\n",
    "        pred_flat = pred.view(pred.size(0), pred.size(1), -1)\n",
    "        target_flat = target.view(target.size(0), target.size(1), -1)\n",
    "\n",
    "        intersection = (pred_flat * target_flat).sum(dim=2)\n",
    "        union = pred_flat.sum(dim=2) + target_flat.sum(dim=2)\n",
    "\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Compute BCE loss without reduction\n",
    "        bce = self.bce(pred, target)  # Shape: (B, C, H, W)\n",
    "        \n",
    "        # Apply class weights if available\n",
    "        if self.class_weights is not None:\n",
    "            # Reshape weights for broadcasting: (1, C, 1, 1)\n",
    "            weights = self.class_weights.view(1, -1, 1, 1)\n",
    "            bce = bce * weights\n",
    "        \n",
    "        # Take mean across all dimensions\n",
    "        bce = bce.mean()\n",
    "        \n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8895c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataloader, num_classes) -> torch.Tensor:\n",
    "    \"\"\"Compute inverse frequency weights for class imbalance.\"\"\"\n",
    "    print(\"Computing class weights...\")\n",
    "    class_pixels = torch.zeros(num_classes)\n",
    "    total_pixels = 0\n",
    "    \n",
    "    for _, masks in tqdm(dataloader, desc=\"Analyzing class distribution\"):\n",
    "        for c in range(num_classes):\n",
    "            class_pixels[c] += masks[:, c, :, :].sum()\n",
    "        total_pixels += masks.numel() // num_classes\n",
    "    \n",
    "    # Inverse frequency weighting\n",
    "    class_freq = class_pixels / total_pixels\n",
    "    class_weights = 1.0 / (class_freq + 1e-6)\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes  # Normalize\n",
    "    \n",
    "    print(f\"Class weights: {class_weights.tolist()}\")\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a59a5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, targets, threshold=0.5, num_classes=CLASSES_COUNT):\n",
    "    \"\"\"Compute per-class IoU and mIoU for multi-label segmentation.\"\"\"\n",
    "    preds = torch.sigmoid(preds) > threshold\n",
    "\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = preds[:, c, :, :].bool()\n",
    "        target_c = targets[:, c, :, :].bool()\n",
    "\n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = (pred_c | target_c).sum().float()\n",
    "\n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "            ious.append(iou.item())\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return ious if ious else [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b33a021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision\n",
    "        with autocast(device_type=device.type, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17dc2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Accumulate per-class intersections and unions across ALL batches\n",
    "    num_classes = CLASSES_COUNT\n",
    "    total_intersection = torch.zeros(num_classes)\n",
    "    total_union = torch.zeros(num_classes)\n",
    "\n",
    "    for images, masks in tqdm(dataloader, desc=\"Validating\"):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(device_type=device.type, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute predictions\n",
    "        preds = torch.sigmoid(outputs) > threshold\n",
    "        preds = preds.cpu()\n",
    "        masks_cpu = masks.cpu()\n",
    "        \n",
    "        # Accumulate per-class metrics\n",
    "        for c in range(num_classes):\n",
    "            pred_c = preds[:, c, :, :].bool()\n",
    "            target_c = masks_cpu[:, c, :, :].bool()\n",
    "            \n",
    "            total_intersection[c] += (pred_c & target_c).sum().float()\n",
    "            total_union[c] += (pred_c | target_c).sum().float()\n",
    "\n",
    "    # Compute IoU for each class\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        if total_union[c] > 0:\n",
    "            iou = (total_intersection[c] / total_union[c]).item()\n",
    "            ious.append(iou)\n",
    "        # else: skip classes with no samples\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_miou = np.mean(ious) if ious else 0.0\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"\\nPer-class IoU: {[f'{iou:.4f}' for iou in ious]}\")\n",
    "    print(f\"Classes evaluated: {len(ious)}/{num_classes}\")\n",
    "\n",
    "    return avg_loss, avg_miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc9d0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=NUM_WORKERS > 0,\n",
    "    drop_last=True,  # Important for BatchNorm\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=CONFIG[\"encoder\"],\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=CONFIG[\"in_channels\"],\n",
    "    classes=CONFIG[\"classes\"],\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5110c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = f\"best_model_{BACKBONE_MODEL}{dataset_name}.pth\"\n",
    "\n",
    "def train_model():\n",
    "    # Compute class weights for imbalance handling\n",
    "    \n",
    "    if not USE_PRECOMPUTED_CLASS_WEIGHTS:\n",
    "        # Calculate class weights from the training dataset\n",
    "        class_weights = compute_class_weights(train_loader, CONFIG[\"classes\"])\n",
    "    else:\n",
    "        # Pre-computed class weights based on dataset analysis\n",
    "        class_weights = torch.tensor([\n",
    "            1.3280059099197388,\n",
    "            0.3596874177455902,\n",
    "            0.9668537974357605,\n",
    "            0.29658055305480957,\n",
    "            0.9289456009864807,\n",
    "            0.5627424716949463,\n",
    "            3.245534658432007,\n",
    "            0.311649352312088\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "    # Loss with class weights\n",
    "    criterion = CombinedLoss(\n",
    "        bce_weight=CONFIG[\"bce_weight\"],\n",
    "        dice_weight=CONFIG[\"dice_weight\"],\n",
    "        class_weights=class_weights.to(DEVICE),\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training tracking (defaults) and resume support\n",
    "    start_epoch = 0\n",
    "    best_miou = 0.0\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
    "\n",
    "    # Resume from checkpoint if requested\n",
    "    if RESUME_TRAINING and os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"Resuming training from {BEST_MODEL_PATH}...\")\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "        # Restore model and optimizer/scheduler/scaler states where available\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load optimizer state: {e}\")\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            try:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load scheduler state: {e}\")\n",
    "        if 'scaler_state_dict' in checkpoint and hasattr(scaler, 'load_state_dict'):\n",
    "            try:\n",
    "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load scaler state: {e}\")\n",
    "\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "        best_miou = checkpoint.get('best_miou', 0.0)\n",
    "        patience_counter = checkpoint.get('patience_counter', 0)\n",
    "        history = checkpoint.get('history', history)\n",
    "        print(f\"Resuming from epoch {start_epoch}, best_miou={best_miou:.4f}, patience={patience_counter}\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting Training\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    for epoch in range(start_epoch, CONFIG[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_miou = validate(\n",
    "            model, val_loader, criterion, DEVICE, CONFIG[\"threshold\"]\n",
    "        )\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log results\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_miou\"].append(val_miou)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model (include extra state for resuming)\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if hasattr(scaler, 'state_dict') else None,\n",
    "                'best_miou': best_miou,\n",
    "                'history': history,\n",
    "                'patience_counter': patience_counter,\n",
    "            }, BEST_MODEL_PATH)\n",
    "            print(f\"✓ New best mIoU: {best_miou:.4f} - Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= CONFIG[\"patience\"]:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Complete! Best mIoU: {best_miou:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ffe43e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from best_model_efficientnet-b3_dataset.pth...\n",
      "Resuming from epoch 2, best_miou=0.3451, patience=0\n",
      "\n",
      "==================================================\n",
      "Starting Training\n",
      "==================================================\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:30:50<00:00,  2.16it/s, loss=0.0792]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:10<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3194', '0.4435', '0.2813', '0.3712', '0.2302', '0.6744', '0.2442', '0.2961']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0741\n",
      "Val Loss: 0.0942 | Val mIoU: 0.3576\n",
      "LR: 7.96e-05\n",
      "✓ New best mIoU: 0.3576 - Model saved!\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:26:49<00:00,  2.22it/s, loss=0.0863]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:07<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3074', '0.4857', '0.2908', '0.3779', '0.2260', '0.6853', '0.4324', '0.3020']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0671\n",
      "Val Loss: 0.0877 | Val mIoU: 0.3885\n",
      "LR: 6.58e-05\n",
      "✓ New best mIoU: 0.3885 - Model saved!\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:27:00<00:00,  2.22it/s, loss=0.0292]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:08<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3369', '0.4520', '0.2921', '0.3522', '0.2448', '0.6503', '0.3659', '0.3053']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0614\n",
      "Val Loss: 0.0925 | Val mIoU: 0.3749\n",
      "LR: 5.05e-05\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:27:39<00:00,  2.21it/s, loss=0.0570]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:07<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3467', '0.4974', '0.2683', '0.3860', '0.2194', '0.6804', '0.4659', '0.2831']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0567\n",
      "Val Loss: 0.0880 | Val mIoU: 0.3934\n",
      "LR: 3.52e-05\n",
      "✓ New best mIoU: 0.3934 - Model saved!\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:26:07<00:00,  2.23it/s, loss=0.0358]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3589', '0.5338', '0.2969', '0.3948', '0.2681', '0.6997', '0.3846', '0.3266']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0524\n",
      "Val Loss: 0.0789 | Val mIoU: 0.4080\n",
      "LR: 2.14e-05\n",
      "✓ New best mIoU: 0.4080 - Model saved!\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:27:16<00:00,  2.21it/s, loss=0.0412]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:05<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3604', '0.5717', '0.3041', '0.4081', '0.2627', '0.7215', '0.4609', '0.3387']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0493\n",
      "Val Loss: 0.0724 | Val mIoU: 0.4285\n",
      "LR: 1.05e-05\n",
      "✓ New best mIoU: 0.4285 - Model saved!\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19549/19549 [2:27:05<00:00,  2.22it/s, loss=0.0333]  \n",
      "Validating: 100%|██████████| 2292/2292 [11:05<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class IoU: ['0.3564', '0.5497', '0.3008', '0.4152', '0.2440', '0.7107', '0.4638', '0.3150']\n",
      "Classes evaluated: 8/8\n",
      "Train Loss: 0.0470\n",
      "Val Loss: 0.0773 | Val mIoU: 0.4195\n",
      "LR: 3.42e-06\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 64/19549 [00:28<2:24:06,  2.25it/s, loss=0.0388]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_MODEL:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model, history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     93\u001b[39m val_loss, val_miou = validate(\n\u001b[32m     94\u001b[39m     model, val_loader, criterion, DEVICE, CONFIG[\u001b[33m\"\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, scaler, device)\u001b[39m\n\u001b[32m     14\u001b[39m     outputs = model(images)\n\u001b[32m     15\u001b[39m     loss = criterion(outputs, masks)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m scaler.step(optimizer)\n\u001b[32m     19\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Max\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agrinet-CSzAdQDi-py3.12\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Max\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agrinet-CSzAdQDi-py3.12\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Max\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agrinet-CSzAdQDi-py3.12\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    model, history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "if not TRAIN_MODEL and os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"Loading best model from {BEST_MODEL_PATH}...\")\n",
    "    checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded. Best mIoU from training: {checkpoint.get('best_miou', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(f\"No pre-trained model found at {BEST_MODEL_PATH}. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72fbfa3",
   "metadata": {},
   "source": [
    "# Compare val with predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PALETTE_DICT = {\n",
    "    'background': (150, 75, 0),          # Brown\n",
    "    'double_plant': (255, 200, 0),       # Gold\n",
    "    'drydown': (210, 105, 30),           # Sienna\n",
    "    'endrow': (255, 165, 0),             # Orange\n",
    "    'nutrient_deficiency': (255, 0, 255),# Magenta\n",
    "    'waterway': (0, 191, 255),           # DeepSkyBlue\n",
    "    'water': (0, 128, 255),              # Blue\n",
    "    'planter_skip': (255, 20, 147),      # Pink\n",
    "    'weed_cluster': (0, 100, 0),         # Dark Green\n",
    "}\n",
    "# Build palette list matching model class order (exclude any labels in EXCLUDE_LABELS).\n",
    "palette = [PALETTE_DICT[label] for label in LABELS if label not in EXCLUDE_LABELS]\n",
    "# Ensure palette length matches CLASSES_COUNT (pad with gray if necessary).\n",
    "if len(palette) < CLASSES_COUNT:\n",
    "    palette += [(128, 128, 128)] * (CLASSES_COUNT - len(palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(data: ImageData, threshold: float = 0.5, as_class_index: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Return model prediction for a single `data` item.\"\"\"\n",
    "    model.eval()\n",
    "    img = data.image.load()\n",
    "    img_batch = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_batch)  # (B, C, H, W)\n",
    "        probs = torch.sigmoid(logits)  # (B, C, H, W)\n",
    "\n",
    "    if as_class_index:\n",
    "        # Collapse to single class index per-pixel (not recommended for multi-label),\n",
    "        # but kept for compatibility if needed.\n",
    "        idx = torch.argmax(probs, dim=1)[0].cpu()  # (H, W)\n",
    "        return idx\n",
    "\n",
    "    # For multi-label task return per-class binary masks (C, H, W)\n",
    "    bin_mask = (probs > threshold)[0].cpu()\n",
    "    return bin_mask\n",
    "\n",
    "def display_mask(mask: torch.Tensor, palette: list[tuple[int, int, int]], base_image: torch.Tensor | None = None, alpha: float = 0.6):\n",
    "    \"\"\"Render a colored overlay for either:\n",
    "    - a 2D class-index mask (H, W) where each pixel contains a single class index, or\n",
    "    - a 3D multi-label binary mask (C, H, W) where classes can overlap.\n",
    "    If `base_image` is provided the colors are blended onto it using `alpha`.\"\"\"\n",
    "    # Normalize palette length and prepare arrays\n",
    "    # Case A: 2D index mask (H, W)\n",
    "    if mask.ndim == 2:\n",
    "        h, w = mask.shape\n",
    "        color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else np.array(mask)\n",
    "        for c, color in enumerate(palette):\n",
    "            color_img[mask_np == c] = color\n",
    "        final = color_img\n",
    "    else:\n",
    "        # Case B: multi-label mask (C, H, W)\n",
    "        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else np.array(mask)\n",
    "        # ensure (C, H, W)\n",
    "        if mask_np.shape[0] != len(palette):\n",
    "            # try if shape is (H, W, C) then transpose\n",
    "            if mask_np.ndim == 3 and mask_np.shape[2] == len(palette):\n",
    "                mask_np = np.transpose(mask_np, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError('Mask shape does not match palette length')\n",
    "\n",
    "        C, h, w = mask_np.shape\n",
    "        # If base_image is provided we will blend sequentially; otherwise build a combined color image\n",
    "        color_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "        for c, color in enumerate(palette):\n",
    "            color = np.array(color, dtype=np.float32)\n",
    "            mask_c = mask_np[c].astype(bool)\n",
    "            if mask_c.any():\n",
    "                # Blend class color onto color_img where class present\n",
    "                color_img[mask_c] = (1.0 - alpha) * color_img[mask_c] + alpha * color\n",
    "        # Clip and convert to uint8\n",
    "        final = np.clip(color_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # If base image present, overlay on top of it for better context\n",
    "    if base_image is not None:\n",
    "        if isinstance(base_image, torch.Tensor):\n",
    "            arr = base_image.cpu().numpy()\n",
    "            arr = np.transpose(arr[:3], (1, 2, 0))\n",
    "            if arr.max() <= 1.1:\n",
    "                arr = (arr * 255).astype(np.uint8)\n",
    "            else:\n",
    "                arr = arr.astype(np.uint8)\n",
    "        else:\n",
    "            arr = np.array(base_image)\n",
    "\n",
    "        if arr.shape[0] != final.shape[0] or arr.shape[1] != final.shape[1]:\n",
    "            arr = np.array(PILImage.fromarray(arr).resize((final.shape[1], final.shape[0])))\n",
    "\n",
    "        blended = ((1.0 - alpha) * arr.astype(np.float32) + alpha * final.astype(np.float32)).astype(np.uint8)\n",
    "        from PIL import ImageFilter\n",
    "        pil = PILImage.fromarray(blended).filter(ImageFilter.GaussianBlur(radius=0.7))\n",
    "        display(pil)\n",
    "        return\n",
    "\n",
    "    from PIL import ImageFilter\n",
    "    pil = PILImage.fromarray(final).filter(ImageFilter.GaussianBlur(radius=1))\n",
    "    display(pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_val_id = random.choice(VAL_IMAGE_IDS)\n",
    "validation_image_data = val_dataset.get_data(random_val_id)\n",
    "\n",
    "print(\"Testing on image ID:\", validation_image_data.image_id)\n",
    "base_img = validation_image_data.image.load()\n",
    "expected_mask = validation_image_data.mask.load()  # (C, H, W)\n",
    "\n",
    "# Make prediction: return multi-label binary mask (C, H, W)\n",
    "pred_bin_mask = predict_mask(validation_image_data, threshold=CONFIG['threshold'], as_class_index=False)\n",
    "\n",
    "# Display predicted mask overlaid on the original image\n",
    "print(f\"Predicted multi-label mask shape: {pred_bin_mask.shape}\")\n",
    "display_mask(pred_bin_mask, palette=palette, base_image=base_img)\n",
    "\n",
    "# Report which labels are detected in prediction with their colors and pixel counts\n",
    "class_names = [label for label in LABELS if label not in EXCLUDE_LABELS]\n",
    "detected_pred = []\n",
    "for i, name in enumerate(class_names):\n",
    "    pixels = int(pred_bin_mask[i].sum().item())\n",
    "    if pixels > 0:\n",
    "        detected_pred.append((name, palette[i], pixels))\n",
    "\n",
    "if detected_pred:\n",
    "    print(\"Predicted labels detected:\")\n",
    "    for name, color, pixels in detected_pred:\n",
    "        print(f\" - {name}: color={color}, pixels={pixels}\")\n",
    "else:\n",
    "    print(\"No labels detected in prediction (threshold may be too high).\")\n",
    "\n",
    "print(\"\\n==================\\n\")\n",
    "\n",
    "# Display the expected (ground-truth) mask for comparison\n",
    "# Ground-truth is multi-label (C, H, W); threshold at 0.5 to get binary presence\n",
    "expected_bin_mask = (expected_mask > 0.5).cpu()\n",
    "print(f\"Expected multi-label mask shape: {expected_bin_mask.shape}\")\n",
    "display_mask(expected_bin_mask, palette=palette, base_image=base_img)\n",
    "\n",
    "# Report which labels are present in ground-truth\n",
    "detected_gt = []\n",
    "for i, name in enumerate(class_names):\n",
    "    pixels = int(expected_bin_mask[i].sum().item())\n",
    "    if pixels > 0:\n",
    "        detected_gt.append((name, palette[i], pixels))\n",
    "\n",
    "if detected_gt:\n",
    "    print(\"Ground-truth labels present:\")\n",
    "    for name, color, pixels in detected_gt:\n",
    "        print(f\" - {name}: color={color}, pixels={pixels}\")\n",
    "else:\n",
    "    print(\"No labels present in ground-truth (unexpected).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrinet-CSzAdQDi-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
