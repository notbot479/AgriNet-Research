{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85288427-7dc3-4053-a2c8-5f87930de968",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebf4c44-1f64-44bb-b5b3-d9af4cbb199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from os import PathLike\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b979f-719c-4023-8037-87a31234b556",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6482a40d-7491-4e9b-a967-b585bfff3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\n",
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021 C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(\"\")\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "KZ_BASE_DIR = os.path.join(ROOT_DIR, \"DJI_202507131523_004\")\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "SMP_MODEL_NAME = \"mobilenet_v2\"\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 512\n",
    "\n",
    "EXCLUDE_LABELS = [\"storm_damage\"]\n",
    "RGBN_CHANNELS = 4\n",
    "CLASSES_COUNT = 1 + 9 - len(EXCLUDE_LABELS) # background + labels - excluded\n",
    "\n",
    "print(ROOT_DIR)\n",
    "print(BASE_DIR, KZ_BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf5685-ae57-445e-ad57-442d5171a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset_1k C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\DJI_202507131523_004\\_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"_dataset_1k\"\n",
    "kz_dataset_name = \"_dataset\"\n",
    "\n",
    "dataset_path = os.path.join(BASE_DIR, dataset_name)\n",
    "kz_dataset_path = os.path.join(KZ_BASE_DIR, kz_dataset_name)\n",
    "\n",
    "dataset_train_path = os.path.join(dataset_path, \"train\")\n",
    "dataset_val_path = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "dataset_test_path = os.path.join(dataset_path, \"test\")\n",
    "kz_dataset_test_path = os.path.join(kz_dataset_path, \"test\")\n",
    "\n",
    "# Train dataset paths\n",
    "IMAGE_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"images\", \"rgbn\"),\n",
    "    os.path.join(dataset_train_path, \"aug_images\", \"rgbn\"),\n",
    "]\n",
    "\n",
    "LABEL_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"labels\"),\n",
    "    os.path.join(dataset_train_path, \"aug_labels\"),\n",
    "]\n",
    "\n",
    "# Validation dataset paths\n",
    "# NOTE: Each label has a folder for each class (labels/class_x, etc.)\n",
    "VAL_IMAGE_RGB_DIR = os.path.join(dataset_val_path, \"images\", \"rgb\")\n",
    "VAL_IMAGE_NIR_DIR = os.path.join(dataset_val_path, \"images\", \"nir\")\n",
    "VAL_LABEL_DIRS = [\n",
    "    os.path.join(dataset_val_path, \"labels\"),\n",
    "]\n",
    "\n",
    "# Test dataset paths\n",
    "INCLUDE_US_TEST_DATASET = True\n",
    "INCLUDE_KZ_TEST_DATASET = False\n",
    "\n",
    "TEST_IMAGE_RGB_DIRS = []\n",
    "TEST_IMAGE_NIR_DIRS = []\n",
    "\n",
    "if INCLUDE_US_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "if INCLUDE_KZ_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "print(dataset_path, kz_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f0df-757f-46bf-be71-90ee09291786",
   "metadata": {},
   "source": [
    "# Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661ebf8c-23fa-4fd2-9ff3-b294102b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_from_numpy(arr: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(arr).long()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_image_from_path(cls, path: str | PathLike) -> torch.Tensor:\n",
    "        arr = np.array(PILImage.open(path))\n",
    "        return cls._load_image_from_numpy(arr)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class Image(ImageSource):\n",
    "    pass\n",
    "\n",
    "class Mask(ImageSource):\n",
    "    pass\n",
    "    \n",
    "@dataclass\n",
    "class ImageData:\n",
    "    image_id: str\n",
    "    image: Image\n",
    "    mask: Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb8927f-1b64-4756-a61e-807196ca7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBImagePlusNIR(Image):\n",
    "    \"\"\" RGB Image with an additional Near-Infrared (NIR) channel.\"\"\"\n",
    "\n",
    "    def __init__(self, rgb_path: str, nir_path: str, normalize=True):\n",
    "        self.rgb_path = rgb_path\n",
    "        self.nir_path = nir_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        rgb = self._load_image_from_path(self.rgb_path).float().permute(2, 0, 1)  # (3, H, W)\n",
    "        nir = self._load_image_from_path(self.nir_path).float().unsqueeze(0)      # (1, H, W)\n",
    "        img = torch.cat([rgb, nir], dim=0)  # (4, H, W)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class RGBNImage(Image):\n",
    "    \"\"\"Png image with 4 channels: Red, Green, Blue, Near-Infrared (NIR).\"\"\"\n",
    "\n",
    "    def __init__(self, rgbn_path: str, normalize=True):\n",
    "        if not self._is_png_image(rgbn_path):\n",
    "            raise ValueError(f\"RGBNImage only supports PNG images. Given file: {rgbn_path}\")\n",
    "        \n",
    "        self.rgbn_path = rgbn_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        img = self._load_image_from_path(self.rgbn_path).float().permute(2, 0, 1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_png_image(file_path: str) -> bool:\n",
    "        return file_path.lower().endswith('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b8735b-92ab-43eb-adda-be42a86d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotMask(Mask):\n",
    "    def __init__(self, *mask_paths: str, sort: bool = True):\n",
    "        self._mask_paths = sorted(mask_paths) if sort else mask_paths\n",
    "        self._labels_count = len(mask_paths)\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        tensors = []\n",
    "\n",
    "        for mask_path in self._mask_paths:\n",
    "            image = self._load_image_from_path(mask_path)\n",
    "            mask = self._get_mask(image)\n",
    "            tensors.append(mask)\n",
    "        \n",
    "        stacked = torch.cat(tensors, dim=0)\n",
    "        \n",
    "        # Add background channel (where all others are 0)\n",
    "        background = (stacked.sum(dim=0, keepdim=True) == 0).long()  # (1, H, W)\n",
    "        return torch.cat([background, stacked], dim=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask(image: torch.Tensor) -> torch.Tensor:\n",
    "        return image.unsqueeze(0)  # (1, H, W)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        cls_name = self.__class__.__name__\n",
    "        labels_count = self._labels_count\n",
    "        return f\"{cls_name}(labels={labels_count})\"\n",
    "\n",
    "class EmptyMask(Mask):\n",
    "    def __init__(self, h: int, w: int, classes: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.classes = classes\n",
    "    \n",
    "    def load(self) -> torch.Tensor:\n",
    "        return torch.zeros(self.classes, self.h, self.w, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4532ce-8175-4302-b461-662c5a6ce719",
   "metadata": {},
   "source": [
    "# ImageIdsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9499d87e-9118-4013-88ee-8eda2112ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIdsParser:\n",
    "    @classmethod\n",
    "    def get_ids_for_dirs(cls, dirs: list[str]) -> list[str]:\n",
    "        ids = []\n",
    "        for d in dirs:\n",
    "            ids.extend(cls.get_ids_for_dir(d))\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ids_for_dir(cls, path: str) -> list[str]:\n",
    "        ids_with_nones = [cls._get_id_from_image_path(p) for p in cls._get_items_by_path(path)]\n",
    "        ids = [i for i in ids_with_nones if i]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_id_from_image_path(cls, path: str) -> str | None:\n",
    "        try:\n",
    "            return path.split(\".\")[0]  # id-with-coords.png\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_items_by_path(cls, path: str) -> list[str]:\n",
    "        try:\n",
    "            return os.listdir(path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] Path not found: {path}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cfb839-da01-4649-a26f-3af1c406fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 image IDs.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(IMAGE_DIRS)\n",
    "print(f\"Found {len(IMAGE_IDS)} image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c598e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 validation image IDs.\n"
     ]
    }
   ],
   "source": [
    "VAL_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs([VAL_IMAGE_RGB_DIR])\n",
    "print(f\"Found {len(VAL_IMAGE_IDS)} validation image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28bea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 test image IDs. Including KZ dataset: False\n"
     ]
    }
   ],
   "source": [
    "TEST_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(TEST_IMAGE_RGB_DIRS)\n",
    "print(f\"Found {len(TEST_IMAGE_IDS)} test image IDs. Including KZ dataset: {INCLUDE_KZ_TEST_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b5c3e-8bc6-4ea7-b25a-035a1b223061",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING IMAGE IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9165ec71-cb8b-4ecd-a829-70b577852a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Based on current IMAGE_IDS and _dataset_processed data fetch about 15000 instances foreach label, ignore all storm damage masks, create plot maybe\n",
    "# 2. Use pre-processed IMAGE_IDS below\n",
    "# 3. Image loader, background class 0 (value 0). Maybe hardcode background creation mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c545ca-8514-4f4f-87c7-cad1a7b42bcd",
   "metadata": {},
   "source": [
    "# Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b956e9db-0f4d-49e5-9e65-05dbfc1fb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(ABC, Dataset):\n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        self._image_ids = image_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self._image_ids[idx]\n",
    "        data = self.get_data(image_id)\n",
    "\n",
    "        img = data.image.load()\n",
    "        mask = data.mask.load()\n",
    "        \n",
    "        # Validate shapes match\n",
    "        if img.shape[-2:] != mask.shape[-2:]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch for {image_id}: \"\n",
    "                f\"image {img.shape} vs mask {mask.shape}\"\n",
    "            )\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def first(self) -> ImageData | None:\n",
    "        try:\n",
    "            first_id = self._image_ids[0]\n",
    "            return self.get_data(first_id)\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576e65-0a6a-4836-ad75-c40f3ed72e55",
   "metadata": {},
   "source": [
    "# File Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f762d464-bf74-4870-a63c-9aaeed5353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSearcher:\n",
    "    def __init__(self, search_paths: list[str], exclude_patterns: list[str] | None = None):\n",
    "        self.search_paths = search_paths\n",
    "        self.exclude_patterns = exclude_patterns or []\n",
    "        self._cache = self._build_index()\n",
    "    \n",
    "    def _build_index(self) -> dict[str, list[str]]:\n",
    "        \"\"\"Build filename -> paths mapping once, excluding specified patterns\"\"\"\n",
    "        index = {}\n",
    "        for root_folder in self.search_paths:\n",
    "            for dirpath, _, filenames in os.walk(root_folder):\n",
    "                \n",
    "                # Check if current dirpath should be excluded\n",
    "                if self._should_exclude(dirpath):\n",
    "                    continue\n",
    "                    \n",
    "                for filename in filenames:\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    \n",
    "                    key = filename.lower()\n",
    "                    if key not in index:\n",
    "                        index[key] = []\n",
    "                    index[key].append(full_path)\n",
    "        return index\n",
    "    \n",
    "    def _should_exclude(self, path: str) -> bool:\n",
    "        \"\"\"Check if path contains any exclude patterns\"\"\"\n",
    "        path_lower = path.lower()\n",
    "        should_exclude = any(pattern.lower() in path_lower for pattern in self.exclude_patterns)\n",
    "\n",
    "        if should_exclude:\n",
    "            print(f\"Exclude path from search: {path}\")\n",
    "        \n",
    "        return should_exclude\n",
    "    \n",
    "    def search(self, file_name: str) -> list[str]:\n",
    "        return self._cache.get(file_name.lower(), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea0521-ac26-4b73-a84d-7eba1a6831b3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05407576-cf2d-4add-9a0c-8bafd4f34008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude path from search: C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset_1k\\train\\labels\\storm_damage\n",
      "Exclude path from search: C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset_1k\\train\\aug_labels\\storm_damage\n",
      "Loaded 2000 instances\n",
      "ImageData(image_id='11IE4DKTR_11556-9586-12068-10098', image=RGBNImage, mask=OneHotMask)\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT - 1  # exclude background\n",
    "    \n",
    "    _image_searcher = FileSearcher(IMAGE_DIRS)\n",
    "    _mask_searcher = FileSearcher(\n",
    "        LABEL_DIRS, \n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "    \n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        super().__init__(image_ids=image_ids)\n",
    "    \n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "    \n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        rgbn_paths = self._image_searcher.search(file_name)\n",
    "\n",
    "        if len(rgbn_paths) == 0 or len(rgbn_paths) > 1:\n",
    "            raise Exception(f\"Expected exactly one RGBN image for ID '{image_id}', found {len(rgbn_paths)}.\")\n",
    "\n",
    "        return RGBNImage(rgbn_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "\n",
    "        self._validate_masks(masks)\n",
    "\n",
    "        return OneHotMask(*masks)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, {expected=}, {found=}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def _test() -> None:\n",
    "    train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "    image_data = train_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} instances\")\n",
    "    \n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b5ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude path from search: C:\\Users\\Max\\Files\\Projects\\AgriNet-Research\\agrinet\\datasets\\Agriculture-Vision-2021\\_dataset_1k\\val\\labels\\storm_damage\n",
      "Loaded 1000 instances in validation dataset\n",
      "ImageData(image_id='138GZZR3U_2368-12251-2880-12763', image=RGBImagePlusNIR, mask=OneHotMask)\n"
     ]
    }
   ],
   "source": [
    "class ValidationDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT - 1\n",
    "\n",
    "    _rgb_searcher = FileSearcher([VAL_IMAGE_RGB_DIR])\n",
    "    _nir_searcher = FileSearcher([VAL_IMAGE_NIR_DIR])\n",
    "    _mask_searcher = FileSearcher(\n",
    "        VAL_LABEL_DIRS,\n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name)\n",
    "        nir_paths = self._nir_searcher.search(file_name)\n",
    "\n",
    "        if len(rgb_paths) != 1 or len(nir_paths) != 1:\n",
    "            raise Exception(\n",
    "                f\"Expected exactly one RGB and one NIR image for ID '{image_id}', \"\n",
    "                f\"found rgb={len(rgb_paths)}, nir={len(nir_paths)}.\"\n",
    "            )\n",
    "\n",
    "        return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_mask_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}' in validation set.\")\n",
    "\n",
    "        self._validate_masks(masks)\n",
    "\n",
    "        return OneHotMask(*masks)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, expected={expected}, found={found}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.jpg\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "def _test() -> None:\n",
    "    val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "    image_data = val_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(val_dataset)} instances in validation dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ce87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 instances in test dataset\n",
      "ImageData(image_id='17FCUVCMI_1728-7254-2240-7766', image=RGBImagePlusNIR, mask=EmptyMask)\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(SegmentationDataset):\n",
    "    _rgb_searcher = FileSearcher(TEST_IMAGE_RGB_DIRS)\n",
    "    _nir_searcher = FileSearcher(TEST_IMAGE_NIR_DIRS)\n",
    "    _dummy_mask_width = _dummy_mask_height = 512\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        image = self._get_image(image_id)\n",
    "        mask = EmptyMask(\n",
    "            h=self._dummy_mask_height,\n",
    "            w=self._dummy_mask_width,\n",
    "            classes=CLASSES_COUNT,\n",
    "        )\n",
    "\n",
    "        return ImageData(image_id=image_id, image=image, mask=mask)\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name_png = f\"{image_id}.png\"\n",
    "        file_name_jpg = f\"{image_id}.jpg\"\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name_jpg) + self._rgb_searcher.search(file_name_png)\n",
    "        nir_paths = self._nir_searcher.search(file_name_jpg) + self._nir_searcher.search(file_name_png)\n",
    "\n",
    "        if len(rgb_paths) == 1 and len(nir_paths) == 1:\n",
    "            return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "        \n",
    "        raise Exception(f\"Failed to resolve test image for ID '{image_id}': rgb={len(rgb_paths)}, nir={len(nir_paths)}\")\n",
    "\n",
    "def _test() -> None:\n",
    "    test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "    image_data = test_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(test_dataset)} instances in test dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef1f41-5b1c-4f15-9e9e-e6718576200c",
   "metadata": {},
   "source": [
    "# Build model with SegmentationModels.Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b96e7e-1462-4225-8ca7-40da70ab1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.metrics import iou_score, get_stats, f1_score\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd2aee77-3ed6-4102-b879-0f102daa4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return smp.DeepLabV3Plus(\n",
    "    encoder_name=SMP_MODEL_NAME,\n",
    "    encoder_weights=None,\n",
    "    in_channels=RGBN_CHANNELS,\n",
    "    classes=CLASSES_COUNT,\n",
    ")\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbe565-c5f2-4cb7-84e3-8bf4b9850385",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4867cc26-80a9-4753-95dd-df0ad654a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_type() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57827020-2595-47f3-aed4-18666065d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected batch size: 8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 8 if get_device_type() == \"cuda\" else 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if get_device_type() == 'cuda' else False,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # Keep workers alive\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,  # Prefetch batches\n",
    ")\n",
    "\n",
    "print(f\"Selected batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de372f-82a4-41e0-8a49-4e2287254c3c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a61900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # Convert one-hot (B, C, H, W) to class indices (B, H, W)\n",
    "    target_idx = torch.argmax(target, dim=1)\n",
    "    return dice_loss(pred, target_idx)\n",
    "\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9511591-fde0-4caf-a69a-f64daa653f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs: torch.Tensor, masks: torch.Tensor):\n",
    "    # outputs: (B, C, H, W) logits\n",
    "    # masks: (B, C, H, W) one-hot\n",
    "    \n",
    "    preds = torch.argmax(outputs, dim=1)  # (B, H, W)\n",
    "    targets = torch.argmax(masks, dim=1)  # (B, H, W)\n",
    "    \n",
    "    # Convert back to one-hot for metrics\n",
    "    preds_onehot = torch.nn.functional.one_hot(preds, num_classes=CLASSES_COUNT).permute(0, 3, 1, 2)\n",
    "    targets_onehot = torch.nn.functional.one_hot(targets, num_classes=CLASSES_COUNT).permute(0, 3, 1, 2)\n",
    "    \n",
    "    tp, fp, fn, tn = get_stats(\n",
    "        preds_onehot,\n",
    "        targets_onehot,\n",
    "        mode='multiclass',\n",
    "        num_classes=CLASSES_COUNT,\n",
    "    )\n",
    "    \n",
    "    iou = iou_score(tp, fp, fn, tn, reduction='micro')\n",
    "    dice = f1_score(tp, fp, fn, tn, reduction='micro')\n",
    "    \n",
    "    return dice.item(), iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "437c9d2f-4bee-46ec-bcb7-edac04d2e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler=None\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgs, masks in loader:\n",
    "        # imgs: (B, 4, H, W), masks: (B, C, H, W) one-hot\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with autocast(device_type):\n",
    "                outputs = model(imgs)\n",
    "                loss = calculate_loss(outputs, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate_one_epoch(model: torch.nn.Module, loader: DataLoader) -> tuple[float, float, float]:\n",
    "    \"\"\"Validate the model for one epoch. Returns loss, dice, iou.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            \n",
    "            dice, iou = calculate_metrics(outputs, masks)\n",
    "            total_dice += dice * imgs.size(0)\n",
    "            total_iou += iou * imgs.size(0)\n",
    "    \n",
    "    dataset_size = len(loader.dataset)\n",
    "    return (total_loss / dataset_size,\n",
    "            total_dice / dataset_size,\n",
    "            total_iou / dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba7cba6-6324-4cb8-b14e-775818523aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device_type = get_device_type()\n",
    "device = torch.device(device_type)\n",
    "print(f\"Using device: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac26037",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scaler = GradScaler(device_type) if device_type == 'cuda' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a843868a-a9f1-4d8e-8862-381cb1c5f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use mIoU metric and calc metric foreach class (background + target classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8015ff3a-34d6-4944-bab6-128720e9d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "MODEL_NAME = f\"best_model_{SMP_MODEL_NAME}.pth\"\n",
    "\n",
    "def train_model() -> None:\n",
    "    best_val_iou = 0.0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "        val_loss, val_dice, val_iou = validate_one_epoch(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "              f\"| Train Loss: {train_loss:.4f} \"\n",
    "              f\"| Val Loss: {val_loss:.4f} \"\n",
    "              f\"| Dice: {val_dice:.4f} \"\n",
    "              f\"| IoU: {val_iou:.4f}\")\n",
    "        \n",
    "        # Save best model only\n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            patience_counter = 0\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_iou': val_iou,\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_NAME)\n",
    "            print(f\"âœ“ Saved new best model (IoU: {val_iou:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30172e13-665b-4749-8ddb-ae869e5fb226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to eval mode on cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "# Robust checkpoint loading: supports raw state_dict or full checkpoint\n",
    "ckpt = torch.load(MODEL_NAME, map_location=device, weights_only=True)\n",
    "\n",
    "if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "    state_dict = ckpt[\"model_state_dict\"]\n",
    "else:\n",
    "    # Assume it's a plain state_dict\n",
    "    state_dict = ckpt\n",
    "\n",
    "# Strip potential 'module.' prefixes (if saved from DataParallel)\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace(\"module.\", \"\")\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "if missing:\n",
    "    print(f\"Warning: missing keys when loading: {missing}\")\n",
    "if unexpected:\n",
    "    print(f\"Warning: unexpected keys when loading: {unexpected}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded and set to eval mode on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5298930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img0 shape: torch.Size([4, 512, 512])\n",
      "gt0 (one-hot) shape: torch.Size([9, 512, 512])\n",
      "pred0 class-index mask shape: torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch from the validation DataLoader\n",
    "imgs, gt_masks = next(iter(test_loader))  # imgs: (B, 4, H, W), gt_masks: (B, C, H, W)\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "# Run the loaded model to get predicted logits\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs)  # (B, 9, H, W)\n",
    "    probs = torch.softmax(logits, dim=1)  # (B, 9, H, W)\n",
    "    pred_masks_idx = torch.argmax(probs, dim=1)  # (B, H, W)\n",
    "\n",
    "# Select the first image/mask from the batch\n",
    "img0 = imgs[0]  # (4, H, W)\n",
    "gt0 = gt_masks[0]  # (9, H, W) one-hot\n",
    "pred0_idx = pred_masks_idx[0]  # (H, W)\n",
    "\n",
    "print(f\"img0 shape: {img0.shape}\")\n",
    "print(f\"gt0 (one-hot) shape: {gt0.shape}\")\n",
    "print(f\"pred0 class-index mask shape: {pred0_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c787f7",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "palette = [\n",
    "    (255, 0, 0),      # class 0 - red\n",
    "    (0, 255, 0),      # class 1 - green\n",
    "    (0, 0, 255),      # class 2 - blue\n",
    "    (255, 255, 0),    # class 3 - yellow\n",
    "    (255, 0, 255),    # class 4 - magenta\n",
    "    (0, 255, 255),    # class 5 - cyan\n",
    "    (255, 165, 0),    # class 6 - orange\n",
    "    (128, 0, 128),    # class 7 - purple\n",
    "    (128, 128, 128),  # class 8 - gray\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "702bb29c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def predict_mask(data: ImageData) -> torch.Tensor:\n",
    "    img = data.image.load()\n",
    "    img_batch = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(img_batch)  # (1, 9, H, W)\n",
    "        # For multiclass: use argmax directly on logits (or after softmax, same result)\n",
    "        pred_idx = torch.argmax(logits, dim=1)  # (1, H, W)\n",
    "    \n",
    "    return pred_idx[0]  # (H, W)\n",
    "\n",
    "def display_mask(image: torch.Tensor, palette: list[tuple[int, int, int]]):\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "\n",
    "    color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    mask_np = image.cpu().numpy()\n",
    "\n",
    "    for c, color in enumerate(palette):\n",
    "        color_img[mask_np == c] = color\n",
    "\n",
    "    display(PILImage.fromarray(color_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac4e9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class-index mask shape: torch.Size([512, 512])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_data = test_dataset.first()\n",
    "\n",
    "pred_idx_mask = predict_mask(test_image_data)\n",
    "print(f\"Predicted class-index mask shape: {pred_idx_mask.shape}\")\n",
    "\n",
    "display_mask(pred_idx_mask, palette=palette)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
