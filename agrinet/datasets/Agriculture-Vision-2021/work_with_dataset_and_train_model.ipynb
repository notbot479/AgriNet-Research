{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85288427-7dc3-4053-a2c8-5f87930de968",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf4c44-1f64-44bb-b5b3-d9af4cbb199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from os import PathLike\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b979f-719c-4023-8037-87a31234b556",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482a40d-7491-4e9b-a967-b585bfff3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(\"\")\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "KZ_BASE_DIR = os.path.join(ROOT_DIR, \"DJI_202507131523_004\")\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "RESUME_TRAINING = True\n",
    "USE_PRECOMPUTED_CLASS_WEIGHTS = True\n",
    "\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 512\n",
    "RGBN_CHANNELS = 4\n",
    "\n",
    "LABELS = [\n",
    "    \"double_plant\",\n",
    "    \"drydown\",\n",
    "    \"endrow\",\n",
    "    \"nutrient_deficiency\",\n",
    "    \"waterway\",\n",
    "    \"water\",\n",
    "    \"planter_skip\",\n",
    "    \"weed_cluster\",\n",
    "    \"storm_damage\",\n",
    "]\n",
    "\n",
    "EXCLUDE_LABELS = [\"storm_damage\"]\n",
    "CLASSES_COUNT = len(LABELS) - len(EXCLUDE_LABELS)\n",
    "\n",
    "print(ROOT_DIR)\n",
    "print(BASE_DIR, KZ_BASE_DIR)\n",
    "print(f\"Classes count: {CLASSES_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf5685-ae57-445e-ad57-442d5171a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"_dataset\"\n",
    "kz_dataset_name = \"_dataset\"\n",
    "\n",
    "dataset_path = os.path.join(BASE_DIR, dataset_name)\n",
    "kz_dataset_path = os.path.join(KZ_BASE_DIR, kz_dataset_name)\n",
    "\n",
    "dataset_train_path = os.path.join(dataset_path, \"train\")\n",
    "dataset_val_path = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "dataset_test_path = os.path.join(dataset_path, \"test\")\n",
    "kz_dataset_test_path = os.path.join(kz_dataset_path, \"test\")\n",
    "\n",
    "# Train dataset paths\n",
    "IMAGE_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"images\", \"rgbn\"),\n",
    "    os.path.join(dataset_train_path, \"aug_images\", \"rgbn\"),\n",
    "]\n",
    "\n",
    "LABEL_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"labels\"),\n",
    "    os.path.join(dataset_train_path, \"aug_labels\"),\n",
    "]\n",
    "\n",
    "# Validation dataset paths\n",
    "# NOTE: Each label has a folder for each class (labels/class_x, etc.)\n",
    "VAL_IMAGE_RGB_DIR = os.path.join(dataset_val_path, \"images\", \"rgb\")\n",
    "VAL_IMAGE_NIR_DIR = os.path.join(dataset_val_path, \"images\", \"nir\")\n",
    "VAL_LABEL_DIRS = [\n",
    "    os.path.join(dataset_val_path, \"labels\"),\n",
    "]\n",
    "\n",
    "# Test dataset paths\n",
    "INCLUDE_US_TEST_DATASET = True\n",
    "INCLUDE_KZ_TEST_DATASET = False\n",
    "\n",
    "TEST_IMAGE_RGB_DIRS = []\n",
    "TEST_IMAGE_NIR_DIRS = []\n",
    "\n",
    "if INCLUDE_US_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "if INCLUDE_KZ_TEST_DATASET:\n",
    "    TEST_IMAGE_RGB_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"rgb\"))\n",
    "    TEST_IMAGE_NIR_DIRS.append(os.path.join(kz_dataset_test_path, \"images\", \"nir\"))\n",
    "\n",
    "print(dataset_path, kz_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f0df-757f-46bf-be71-90ee09291786",
   "metadata": {},
   "source": [
    "# Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ebf8c-23fa-4fd2-9ff3-b294102b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_from_numpy(arr: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(arr.astype(np.float32))\n",
    "\n",
    "    @classmethod\n",
    "    def _load_image_from_path(cls, path: str | PathLike) -> torch.Tensor:\n",
    "        arr = np.array(PILImage.open(path))\n",
    "        return cls._load_image_from_numpy(arr)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class Image(ImageSource):\n",
    "    pass\n",
    "\n",
    "class Mask(ImageSource):\n",
    "    pass\n",
    "    \n",
    "@dataclass\n",
    "class ImageData:\n",
    "    image_id: str\n",
    "    image: Image\n",
    "    mask: Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8927f-1b64-4756-a61e-807196ca7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBImagePlusNIR(Image):\n",
    "    \"\"\" RGB Image with an additional Near-Infrared (NIR) channel.\"\"\"\n",
    "\n",
    "    def __init__(self, rgb_path: str, nir_path: str, normalize=True):\n",
    "        self.rgb_path = rgb_path\n",
    "        self.nir_path = nir_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        rgb = self._load_image_from_path(self.rgb_path).permute(2, 0, 1)  # (3, H, W)\n",
    "        nir = self._load_image_from_path(self.nir_path).unsqueeze(0)      # (1, H, W)\n",
    "        img = torch.cat([rgb, nir], dim=0)  # (4, H, W)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class RGBNImage(Image):\n",
    "    \"\"\"Png image with 4 channels: Red, Green, Blue, Near-Infrared (NIR).\"\"\"\n",
    "\n",
    "    def __init__(self, rgbn_path: str, normalize=True):\n",
    "        if not self._is_png_image(rgbn_path):\n",
    "            raise ValueError(f\"RGBNImage only supports PNG images. Given file: {rgbn_path}\")\n",
    "        \n",
    "        self.rgbn_path = rgbn_path\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        img = self._load_image_from_path(self.rgbn_path).permute(2, 0, 1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_png_image(file_path: str) -> bool:\n",
    "        return file_path.lower().endswith('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8735b-92ab-43eb-adda-be42a86d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMask(Mask):\n",
    "    \"\"\"Multi-label mask where labels CAN overlap (no mutual exclusivity).\"\"\"\n",
    "    \n",
    "    def __init__(self, *mask_paths: str, labels_order: list[str] | None = None):\n",
    "        paths = list(mask_paths)\n",
    "        if labels_order:\n",
    "            self._mask_paths = self._sort_by_labels(paths, labels_order)\n",
    "        else:\n",
    "            self._mask_paths = sorted(paths)\n",
    "        self._labels_count = len(self._mask_paths)\n",
    "\n",
    "    def get_label_order(self) -> list[str]:\n",
    "        return [self._get_label_from_path(p) for p in self._mask_paths]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_label_from_path(path: str) -> str:\n",
    "        return os.path.basename(os.path.dirname(path)).lower()\n",
    "\n",
    "    @classmethod\n",
    "    def _sort_by_labels(cls, mask_paths: list[str], labels_order: list[str]) -> list[str]:\n",
    "        path_by_label = {}\n",
    "        for path in mask_paths:\n",
    "            label = cls._get_label_from_path(path)\n",
    "            path_by_label[label] = path\n",
    "\n",
    "        sorted_paths = []\n",
    "        for label in labels_order:\n",
    "            label_lower = label.lower()\n",
    "            if label_lower in path_by_label:\n",
    "                sorted_paths.append(path_by_label[label_lower])\n",
    "        return sorted_paths\n",
    "\n",
    "    def load(self) -> torch.Tensor:\n",
    "        tensors = []\n",
    "        for mask_path in self._mask_paths:\n",
    "            image = self._load_image_from_path(mask_path)\n",
    "            mask = (image > 0).float().unsqueeze(0)  # (1, H, W)\n",
    "            tensors.append(mask)\n",
    "        \n",
    "        return torch.cat(tensors, dim=0)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(classes={self._labels_count})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyMask(Mask):\n",
    "    def __init__(self, h: int, w: int, classes: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.classes = classes\n",
    "\n",
    "    def load(self) -> torch.Tensor:\n",
    "        return torch.zeros(self.classes, self.h, self.w, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4532ce-8175-4302-b461-662c5a6ce719",
   "metadata": {},
   "source": [
    "# ImageIdsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499d87e-9118-4013-88ee-8eda2112ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIdsParser:\n",
    "    @classmethod\n",
    "    def get_ids_for_dirs(cls, dirs: list[str]) -> list[str]:\n",
    "        ids = []\n",
    "        for d in dirs:\n",
    "            ids.extend(cls.get_ids_for_dir(d))\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ids_for_dir(cls, path: str) -> list[str]:\n",
    "        ids_with_nones = [cls._get_id_from_image_path(p) for p in cls._get_items_by_path(path)]\n",
    "        ids = [i for i in ids_with_nones if i]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_id_from_image_path(cls, path: str) -> str | None:\n",
    "        try:\n",
    "            return path.split(\".\")[0]  # id-with-coords.png\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_items_by_path(cls, path: str) -> list[str]:\n",
    "        try:\n",
    "            return os.listdir(path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] Path not found: {path}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfb839-da01-4649-a26f-3af1c406fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(IMAGE_DIRS)\n",
    "print(f\"Found {len(IMAGE_IDS)} image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c598e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs([VAL_IMAGE_RGB_DIR])\n",
    "print(f\"Found {len(VAL_IMAGE_IDS)} validation image IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(TEST_IMAGE_RGB_DIRS)\n",
    "print(f\"Found {len(TEST_IMAGE_IDS)} test image IDs. Including KZ dataset: {INCLUDE_KZ_TEST_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c545ca-8514-4f4f-87c7-cad1a7b42bcd",
   "metadata": {},
   "source": [
    "# Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956e9db-0f4d-49e5-9e65-05dbfc1fb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(ABC, Dataset):\n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        self._image_ids = image_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self._image_ids[idx]\n",
    "        data = self.get_data(image_id)\n",
    "\n",
    "        img = data.image.load()\n",
    "        mask = data.mask.load()\n",
    "        \n",
    "        # Validate shapes match\n",
    "        if img.shape[-2:] != mask.shape[-2:]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch for {image_id}: \"\n",
    "                f\"image {img.shape} vs mask {mask.shape}\"\n",
    "            )\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def first(self) -> ImageData | None:\n",
    "        try:\n",
    "            first_id = self._image_ids[0]\n",
    "            return self.get_data(first_id)\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576e65-0a6a-4836-ad75-c40f3ed72e55",
   "metadata": {},
   "source": [
    "# File Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762d464-bf74-4870-a63c-9aaeed5353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSearcher:\n",
    "    def __init__(self, search_paths: list[str], exclude_patterns: list[str] | None = None):\n",
    "        self.search_paths = search_paths\n",
    "        self.exclude_patterns = exclude_patterns or []\n",
    "        self._cache = self._build_index()\n",
    "    \n",
    "    def _build_index(self) -> dict[str, list[str]]:\n",
    "        \"\"\"Build filename -> paths mapping once, excluding specified patterns\"\"\"\n",
    "        index = {}\n",
    "        for root_folder in self.search_paths:\n",
    "            for dirpath, _, filenames in os.walk(root_folder):\n",
    "                \n",
    "                # Check if current dirpath should be excluded\n",
    "                if self._should_exclude(dirpath):\n",
    "                    continue\n",
    "                    \n",
    "                for filename in filenames:\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    \n",
    "                    key = filename.lower()\n",
    "                    if key not in index:\n",
    "                        index[key] = []\n",
    "                    index[key].append(full_path)\n",
    "        return index\n",
    "    \n",
    "    def _should_exclude(self, path: str) -> bool:\n",
    "        \"\"\"Check if path contains any exclude patterns\"\"\"\n",
    "        path_lower = path.lower()\n",
    "        should_exclude = any(pattern.lower() in path_lower for pattern in self.exclude_patterns)\n",
    "\n",
    "        if should_exclude:\n",
    "            print(f\"Exclude path from search: {path}\")\n",
    "        \n",
    "        return should_exclude\n",
    "    \n",
    "    def search(self, file_name: str) -> list[str]:\n",
    "        return self._cache.get(file_name.lower(), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea0521-ac26-4b73-a84d-7eba1a6831b3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05407576-cf2d-4add-9a0c-8bafd4f34008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT\n",
    "    \n",
    "    _image_searcher = FileSearcher(IMAGE_DIRS)\n",
    "    _mask_searcher = FileSearcher(\n",
    "        LABEL_DIRS, \n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "    \n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        super().__init__(image_ids=image_ids)\n",
    "    \n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "    \n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        rgbn_paths = self._image_searcher.search(file_name)\n",
    "\n",
    "        if len(rgbn_paths) == 0 or len(rgbn_paths) > 1:\n",
    "            raise Exception(f\"Expected exactly one RGBN image for ID '{image_id}', found {len(rgbn_paths)}.\")\n",
    "\n",
    "        return RGBNImage(rgbn_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "        self._validate_masks(masks)\n",
    "        return MultiLabelMask(*masks, labels_order=LABELS)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, {expected=}, {found=}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def _test() -> None:\n",
    "    train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "    image_data = train_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} instances\")\n",
    "    \n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "        print(\"Labels order in mask tensor\")\n",
    "        print(image_data.mask.get_label_order())\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = CLASSES_COUNT\n",
    "\n",
    "    _rgb_searcher = FileSearcher([VAL_IMAGE_RGB_DIR])\n",
    "    _nir_searcher = FileSearcher([VAL_IMAGE_NIR_DIR])\n",
    "    _mask_searcher = FileSearcher(\n",
    "        VAL_LABEL_DIRS,\n",
    "        exclude_patterns=EXCLUDE_LABELS,\n",
    "    )\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name)\n",
    "        nir_paths = self._nir_searcher.search(file_name)\n",
    "\n",
    "        if len(rgb_paths) != 1 or len(nir_paths) != 1:\n",
    "            raise Exception(\n",
    "                f\"Expected exactly one RGB and one NIR image for ID '{image_id}', \"\n",
    "                f\"found rgb={len(rgb_paths)}, nir={len(nir_paths)}.\"\n",
    "            )\n",
    "\n",
    "        return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_mask_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "        self._validate_masks(masks)\n",
    "        return MultiLabelMask(*masks, labels_order=LABELS)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(f\"Failed parse masks, expected={expected}, found={found}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.jpg\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "def _test() -> None:\n",
    "    val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "    image_data = val_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(val_dataset)} instances in validation dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(SegmentationDataset):\n",
    "    _rgb_searcher = FileSearcher(TEST_IMAGE_RGB_DIRS)\n",
    "    _nir_searcher = FileSearcher(TEST_IMAGE_NIR_DIRS)\n",
    "    _dummy_mask_width = IMAGE_WIDTH\n",
    "    _dummy_mask_height = IMAGE_HEIGHT\n",
    "\n",
    "    def __init__(self, image_ids: list[str]) -> None:\n",
    "        super().__init__(image_ids=image_ids)\n",
    "\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        image = self._get_image(image_id)\n",
    "        mask = EmptyMask(\n",
    "            h=self._dummy_mask_height,\n",
    "            w=self._dummy_mask_width,\n",
    "            classes=CLASSES_COUNT,\n",
    "        )\n",
    "\n",
    "        return ImageData(image_id=image_id, image=image, mask=mask)\n",
    "\n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name_png = f\"{image_id}.png\"\n",
    "        file_name_jpg = f\"{image_id}.jpg\"\n",
    "\n",
    "        rgb_paths = self._rgb_searcher.search(file_name_jpg) + self._rgb_searcher.search(file_name_png)\n",
    "        nir_paths = self._nir_searcher.search(file_name_jpg) + self._nir_searcher.search(file_name_png)\n",
    "\n",
    "        if len(rgb_paths) == 1 and len(nir_paths) == 1:\n",
    "            return RGBImagePlusNIR(rgb_path=rgb_paths[0], nir_path=nir_paths[0])\n",
    "        \n",
    "        raise Exception(f\"Failed to resolve test image for ID '{image_id}': rgb={len(rgb_paths)}, nir={len(nir_paths)}\")\n",
    "\n",
    "def _test() -> None:\n",
    "    test_dataset = TestDataset(image_ids=TEST_IMAGE_IDS)\n",
    "    image_data = test_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(test_dataset)} instances in test dataset\")\n",
    "\n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de372f-82a4-41e0-8a49-4e2287254c3c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = min(0, os.cpu_count() or 1)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1283d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_MODEL = \"efficientnet-b3\"\n",
    "\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 8 if DEVICE.type == \"cuda\" else 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \n",
    "    # Model\n",
    "    \"encoder\": BACKBONE_MODEL,\n",
    "    \"in_channels\": 4,  # RGBN\n",
    "    \"classes\": CLASSES_COUNT,\n",
    "    \n",
    "    # Loss\n",
    "    \"bce_weight\": 0.5,\n",
    "    \"dice_weight\": 0.5,\n",
    "    \n",
    "    # Early stopping\n",
    "    \"patience\": 10,\n",
    "    \n",
    "    # Threshold for predictions\n",
    "    \"threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined BCE + Dice Loss for multi-label segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        \n",
    "        # Store class weights for manual weighting\n",
    "        if class_weights is not None:\n",
    "            if not isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "            # Ensure it's 1D and detached\n",
    "            self.register_buffer('class_weights', class_weights.detach().clone().view(-1))\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Use BCE without pos_weight, we'll apply weights manually\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def dice_loss(self, pred, target, smooth=1.0):\n",
    "        \"\"\"Dice loss for multi-label.\"\"\"\n",
    "        pred = torch.sigmoid(pred)\n",
    "\n",
    "        # Flatten per class\n",
    "        pred_flat = pred.view(pred.size(0), pred.size(1), -1)\n",
    "        target_flat = target.view(target.size(0), target.size(1), -1)\n",
    "\n",
    "        intersection = (pred_flat * target_flat).sum(dim=2)\n",
    "        union = pred_flat.sum(dim=2) + target_flat.sum(dim=2)\n",
    "\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Compute BCE loss without reduction\n",
    "        bce = self.bce(pred, target)  # Shape: (B, C, H, W)\n",
    "        \n",
    "        # Apply class weights if available\n",
    "        if self.class_weights is not None:\n",
    "            # Reshape weights for broadcasting: (1, C, 1, 1)\n",
    "            weights = self.class_weights.view(1, -1, 1, 1)\n",
    "            bce = bce * weights\n",
    "        \n",
    "        # Take mean across all dimensions\n",
    "        bce = bce.mean()\n",
    "        \n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataloader, num_classes) -> torch.Tensor:\n",
    "    \"\"\"Compute inverse frequency weights for class imbalance.\"\"\"\n",
    "    print(\"Computing class weights...\")\n",
    "    class_pixels = torch.zeros(num_classes)\n",
    "    total_pixels = 0\n",
    "    \n",
    "    for _, masks in tqdm(dataloader, desc=\"Analyzing class distribution\"):\n",
    "        for c in range(num_classes):\n",
    "            class_pixels[c] += masks[:, c, :, :].sum()\n",
    "        total_pixels += masks.numel() // num_classes\n",
    "    \n",
    "    # Inverse frequency weighting\n",
    "    class_freq = class_pixels / total_pixels\n",
    "    class_weights = 1.0 / (class_freq + 1e-6)\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes  # Normalize\n",
    "    \n",
    "    print(f\"Class weights: {class_weights.tolist()}\")\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, targets, threshold=0.5, num_classes=CLASSES_COUNT):\n",
    "    \"\"\"Compute per-class IoU and mIoU for multi-label segmentation.\"\"\"\n",
    "    preds = torch.sigmoid(preds) > threshold\n",
    "\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = preds[:, c, :, :].bool()\n",
    "        target_c = targets[:, c, :, :].bool()\n",
    "\n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = (pred_c | target_c).sum().float()\n",
    "\n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "            ious.append(iou.item())\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return ious if ious else [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision\n",
    "        with autocast(device_type=device.type, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Accumulate per-class intersections and unions across ALL batches\n",
    "    num_classes = CLASSES_COUNT\n",
    "    total_intersection = torch.zeros(num_classes)\n",
    "    total_union = torch.zeros(num_classes)\n",
    "\n",
    "    for images, masks in tqdm(dataloader, desc=\"Validating\"):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(device_type=device.type, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute predictions\n",
    "        preds = torch.sigmoid(outputs) > threshold\n",
    "        preds = preds.cpu()\n",
    "        masks_cpu = masks.cpu()\n",
    "        \n",
    "        # Accumulate per-class metrics\n",
    "        for c in range(num_classes):\n",
    "            pred_c = preds[:, c, :, :].bool()\n",
    "            target_c = masks_cpu[:, c, :, :].bool()\n",
    "            \n",
    "            total_intersection[c] += (pred_c & target_c).sum().float()\n",
    "            total_union[c] += (pred_c | target_c).sum().float()\n",
    "\n",
    "    # Compute IoU for each class\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        if total_union[c] > 0:\n",
    "            iou = (total_intersection[c] / total_union[c]).item()\n",
    "            ious.append(iou)\n",
    "        # else: skip classes with no samples\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_miou = np.mean(ious) if ious else 0.0\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"\\nPer-class IoU: {[f'{iou:.4f}' for iou in ious]}\")\n",
    "    print(f\"Classes evaluated: {len(ious)}/{num_classes}\")\n",
    "\n",
    "    return avg_loss, avg_miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "val_dataset = ValidationDataset(image_ids=VAL_IMAGE_IDS)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=NUM_WORKERS > 0,\n",
    "    drop_last=True,  # Important for BatchNorm\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=CONFIG[\"encoder\"],\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=CONFIG[\"in_channels\"],\n",
    "    classes=CONFIG[\"classes\"],\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = f\"best_model_{BACKBONE_MODEL}{dataset_name}.pth\"\n",
    "\n",
    "def train_model():\n",
    "    # Compute class weights for imbalance handling\n",
    "    \n",
    "    if not USE_PRECOMPUTED_CLASS_WEIGHTS:\n",
    "        # Calculate class weights from the training dataset\n",
    "        class_weights = compute_class_weights(train_loader, CONFIG[\"classes\"])\n",
    "    else:\n",
    "        # Pre-computed class weights based on dataset analysis\n",
    "        class_weights = torch.tensor([\n",
    "            1.3280059099197388,\n",
    "            0.3596874177455902,\n",
    "            0.9668537974357605,\n",
    "            0.29658055305480957,\n",
    "            0.9289456009864807,\n",
    "            0.5627424716949463,\n",
    "            3.245534658432007,\n",
    "            0.311649352312088\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "    # Loss with class weights\n",
    "    criterion = CombinedLoss(\n",
    "        bce_weight=CONFIG[\"bce_weight\"],\n",
    "        dice_weight=CONFIG[\"dice_weight\"],\n",
    "        class_weights=class_weights.to(DEVICE),\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training tracking (defaults) and resume support\n",
    "    start_epoch = 0\n",
    "    best_miou = 0.0\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
    "\n",
    "    # Resume from checkpoint if requested\n",
    "    if RESUME_TRAINING and os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"Resuming training from {BEST_MODEL_PATH}...\")\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "        \n",
    "        # Load epoch to resume from (default to 0 if not found)\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "\n",
    "        # Restore model and optimizer/scheduler/scaler states where available\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load optimizer state: {e}\")\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            try:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load scheduler state: {e}\")\n",
    "        if 'scaler_state_dict' in checkpoint and hasattr(scaler, 'load_state_dict'):\n",
    "            try:\n",
    "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to load scaler state: {e}\")\n",
    "\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "        best_miou = checkpoint.get('best_miou', 0.0)\n",
    "        patience_counter = checkpoint.get('patience_counter', 0)\n",
    "        history = checkpoint.get('history', history)\n",
    "        print(f\"Resuming from epoch {start_epoch}, best_miou={best_miou:.4f}, patience={patience_counter}\")\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=CONFIG[\"epochs\"] - start_epoch,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting Training\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    for epoch in range(start_epoch, CONFIG[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_miou = validate(\n",
    "            model, val_loader, criterion, DEVICE, CONFIG[\"threshold\"]\n",
    "        )\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log results\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_miou\"].append(val_miou)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model (include extra state for resuming)\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if hasattr(scaler, 'state_dict') else None,\n",
    "                'best_miou': best_miou,\n",
    "                'history': history,\n",
    "                'patience_counter': patience_counter,\n",
    "            }, BEST_MODEL_PATH)\n",
    "            print(f\"âœ“ New best mIoU: {best_miou:.4f} - Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= CONFIG[\"patience\"]:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Complete! Best mIoU: {best_miou:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    model, history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "if not TRAIN_MODEL and os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"Loading best model from {BEST_MODEL_PATH}...\")\n",
    "    checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded. Best mIoU from training: {checkpoint.get('best_miou', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(f\"No pre-trained model found at {BEST_MODEL_PATH}. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72fbfa3",
   "metadata": {},
   "source": [
    "# Compare val with predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PALETTE_DICT = {\n",
    "    'background': (150, 75, 0),          # Brown\n",
    "    'double_plant': (255, 200, 0),       # Gold\n",
    "    'drydown': (210, 105, 30),           # Sienna\n",
    "    'endrow': (255, 165, 0),             # Orange\n",
    "    'nutrient_deficiency': (255, 0, 255),# Magenta\n",
    "    'waterway': (0, 191, 255),           # DeepSkyBlue\n",
    "    'water': (0, 128, 255),              # Blue\n",
    "    'planter_skip': (255, 20, 147),      # Pink\n",
    "    'weed_cluster': (0, 100, 0),         # Dark Green\n",
    "}\n",
    "# Build palette list matching model class order (exclude any labels in EXCLUDE_LABELS).\n",
    "palette = [PALETTE_DICT[label] for label in LABELS if label not in EXCLUDE_LABELS]\n",
    "# Ensure palette length matches CLASSES_COUNT (pad with gray if necessary).\n",
    "if len(palette) < CLASSES_COUNT:\n",
    "    palette += [(128, 128, 128)] * (CLASSES_COUNT - len(palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "\n",
    "def predict_mask(data: ImageData, threshold: float = 0.5, as_class_index: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Return model prediction for a single `data` item.\"\"\"\n",
    "    model.eval()\n",
    "    img = data.image.load()\n",
    "    img_batch = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_batch)  # (B, C, H, W)\n",
    "        probs = torch.sigmoid(logits)  # (B, C, H, W)\n",
    "\n",
    "    if as_class_index:\n",
    "        # Collapse to single class index per-pixel (not recommended for multi-label),\n",
    "        # but kept for compatibility if needed.\n",
    "        idx = torch.argmax(probs, dim=1)[0].cpu()  # (H, W)\n",
    "        return idx\n",
    "\n",
    "    # For multi-label task return per-class binary masks (C, H, W)\n",
    "    bin_mask = (probs > threshold)[0].cpu()\n",
    "    return bin_mask\n",
    "\n",
    "def display_mask(mask: torch.Tensor, palette: list[tuple[int, int, int]], base_image: torch.Tensor | None = None, alpha: float = 0.6):\n",
    "    # Case A: 2D index mask (H, W)\n",
    "    if mask.ndim == 2:\n",
    "        h, w = mask.shape\n",
    "        color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else np.array(mask)\n",
    "        for c, color in enumerate(palette):\n",
    "            color_img[mask_np == c] = color\n",
    "        final = color_img\n",
    "        mask_present = mask_np > 0  # Track where masks exist\n",
    "    else:\n",
    "        # Case B: multi-label mask (C, H, W)\n",
    "        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else np.array(mask)\n",
    "        \n",
    "        if mask_np.shape[0] != len(palette):\n",
    "            if mask_np.ndim == 3 and mask_np.shape[2] == len(palette):\n",
    "                mask_np = np.transpose(mask_np, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError('Mask shape does not match palette length')\n",
    "\n",
    "        C, h, w = mask_np.shape\n",
    "        color_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "        mask_present = np.zeros((h, w), dtype=bool)  # Track any mask\n",
    "        \n",
    "        for c, color in enumerate(palette):\n",
    "            color = np.array(color, dtype=np.float32)\n",
    "            mask_c = mask_np[c].astype(bool)\n",
    "            if mask_c.any():\n",
    "                mask_present |= mask_c  # Update tracking\n",
    "                # Don't apply alpha here - just accumulate colors\n",
    "                color_img[mask_c] = color_img[mask_c] + color  # or use max/average strategy\n",
    "        \n",
    "        # Clip and convert to uint8\n",
    "        final = np.clip(color_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # If base image present, overlay on top of it\n",
    "    if base_image is not None:\n",
    "        if isinstance(base_image, torch.Tensor):\n",
    "            arr = base_image.cpu().numpy()\n",
    "            arr = np.transpose(arr[:3], (1, 2, 0))\n",
    "            if arr.max() <= 1.1:\n",
    "                arr = (arr * 255).astype(np.uint8)\n",
    "            else:\n",
    "                arr = arr.astype(np.uint8)\n",
    "        else:\n",
    "            arr = np.array(base_image)\n",
    "\n",
    "        if arr.shape[0] != final.shape[0] or arr.shape[1] != final.shape[1]:\n",
    "            arr = np.array(PILImage.fromarray(arr).resize((final.shape[1], final.shape[0])))\n",
    "\n",
    "        # Only blend where masks exist\n",
    "        blended = arr.copy()\n",
    "        blended[mask_present] = (\n",
    "            (1.0 - alpha) * arr[mask_present].astype(np.float32) + \n",
    "            alpha * final[mask_present].astype(np.float32)\n",
    "        ).astype(np.uint8)\n",
    "        \n",
    "        pil = PILImage.fromarray(blended).filter(ImageFilter.GaussianBlur(radius=0.7))\n",
    "        display(pil)\n",
    "        return\n",
    "\n",
    "    pil = PILImage.fromarray(final).filter(ImageFilter.GaussianBlur(radius=1))\n",
    "    display(pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_val_id = random.choice(VAL_IMAGE_IDS)\n",
    "validation_image_data = val_dataset.get_data(random_val_id)\n",
    "\n",
    "print(\"Testing on image ID:\", validation_image_data.image_id)\n",
    "base_img = validation_image_data.image.load()\n",
    "expected_mask = validation_image_data.mask.load()  # (C, H, W)\n",
    "\n",
    "# Make prediction: return multi-label binary mask (C, H, W)\n",
    "pred_bin_mask = predict_mask(validation_image_data, threshold=CONFIG['threshold'], as_class_index=False)\n",
    "\n",
    "# Display predicted mask overlaid on the original image\n",
    "print(f\"Predicted multi-label mask shape: {pred_bin_mask.shape}\")\n",
    "display_mask(pred_bin_mask, palette=palette, base_image=base_img)\n",
    "\n",
    "# Report which labels are detected in prediction with their colors and pixel counts\n",
    "class_names = [label for label in LABELS if label not in EXCLUDE_LABELS]\n",
    "detected_pred = []\n",
    "for i, name in enumerate(class_names):\n",
    "    pixels = int(pred_bin_mask[i].sum().item())\n",
    "    if pixels > 0:\n",
    "        detected_pred.append((name, palette[i], pixels))\n",
    "\n",
    "if detected_pred:\n",
    "    print(\"Predicted labels detected:\")\n",
    "    for name, color, pixels in detected_pred:\n",
    "        print(f\" - {name}: color={color}, pixels={pixels}\")\n",
    "else:\n",
    "    print(\"No labels detected in prediction (threshold may be too high).\")\n",
    "\n",
    "print(\"\\n==================\\n\")\n",
    "\n",
    "# Display the expected (ground-truth) mask for comparison\n",
    "# Ground-truth is multi-label (C, H, W); threshold at 0.5 to get binary presence\n",
    "expected_bin_mask = (expected_mask > 0.5).cpu()\n",
    "print(f\"Expected multi-label mask shape: {expected_bin_mask.shape}\")\n",
    "display_mask(expected_bin_mask, palette=palette, base_image=base_img)\n",
    "\n",
    "# Report which labels are present in ground-truth\n",
    "detected_gt = []\n",
    "for i, name in enumerate(class_names):\n",
    "    pixels = int(expected_bin_mask[i].sum().item())\n",
    "    if pixels > 0:\n",
    "        detected_gt.append((name, palette[i], pixels))\n",
    "\n",
    "if detected_gt:\n",
    "    print(\"Ground-truth labels present:\")\n",
    "    for name, color, pixels in detected_gt:\n",
    "        print(f\" - {name}: color={color}, pixels={pixels}\")\n",
    "else:\n",
    "    print(\"No labels present in ground-truth (unexpected).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrinet-CSzAdQDi-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
