{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85288427-7dc3-4053-a2c8-5f87930de968",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bebf4c44-1f64-44bb-b5b3-d9af4cbb199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from os import PathLike\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b979f-719c-4023-8037-87a31234b556",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6482a40d-7491-4e9b-a967-b585bfff3832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/max/D09450A794509238/AgriNet-Research/agrinet/datasets/Agriculture-Vision-2021'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(\"\")\n",
    "\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "85bf5685-ae57-445e-ad57-442d5171a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/max/D09450A794509238/AgriNet-Research/agrinet/datasets/Agriculture-Vision-2021/_dataset_mini\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"_dataset_mini\"\n",
    "dataset_path = os.path.join(BASE_DIR, dataset_name)\n",
    "dataset_train_path = os.path.join(dataset_path, \"train\")\n",
    "dataset_val_path = os.path.join(dataset_path, \"val\")\n",
    "dataset_test_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "IMAGE_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"images\", \"rgbn\"),\n",
    "    os.path.join(dataset_train_path, \"aug_images\", \"rgbn\"),\n",
    "]\n",
    "\n",
    "# NOTE: Each label has a folder for each class (labels/class_x, etc.)\n",
    "LABEL_DIRS = [\n",
    "    os.path.join(dataset_train_path, \"labels\"),\n",
    "    os.path.join(dataset_train_path, \"aug_labels\"),\n",
    "]\n",
    "\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f0df-757f-46bf-be71-90ee09291786",
   "metadata": {},
   "source": [
    "# Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "661ebf8c-23fa-4fd2-9ff3-b294102b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image_from_numpy(arr: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(arr).long()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_image_from_path(cls, path: str | PathLike) -> torch.Tensor:\n",
    "        arr = np.array(PILImage.open(path))\n",
    "        return cls._load_image_from_numpy(arr)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class Image(ImageSource):\n",
    "    pass\n",
    "\n",
    "class Mask(ImageSource):\n",
    "    pass\n",
    "    \n",
    "@dataclass\n",
    "class ImageData:\n",
    "    image_id: str\n",
    "    image: Image\n",
    "    mask: Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "fcb8927f-1b64-4756-a61e-807196ca7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBImagePlusNIR(Image):\n",
    "    \"\"\" RGB Image with an additional Near-Infrared (NIR) channel.\"\"\"\n",
    "\n",
    "    def __init__(self, rgb_path: str, nir_path: str):\n",
    "        self.rgb_path = rgb_path\n",
    "        self.nir_path = nir_path\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        rgb = self._load_image_from_path(self.rgb_path).float().permute(2, 0, 1)  # (3, H, W)\n",
    "        nir = self._load_image_from_path(self.nir_path).float().unsqueeze(0)      # (1, H, W)\n",
    "        return torch.cat([rgb, nir], dim=0)  # (4, H, W)\n",
    "\n",
    "\n",
    "class RGBNImage(Image):\n",
    "    \"\"\"Png image with 4 channels: Red, Green, Blue, Near-Infrared (NIR).\"\"\"\n",
    "\n",
    "    def __init__(self, rgbn_path: str):\n",
    "        if not self._is_png_image(rgbn_path):\n",
    "            raise ValueError(f\"RGBNImage only supports PNG images. Given file: {rgbn_path}\")\n",
    "\n",
    "        self.rgbn_path = rgbn_path\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        return self._load_image_from_path(self.rgbn_path).float().permute(2, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_png_image(file_path: str) -> bool:\n",
    "        return file_path.lower().endswith('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "01b8735b-92ab-43eb-adda-be42a86d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotMask(Mask):\n",
    "    def __init__(self, *mask_paths: str):\n",
    "        self._mask_paths = mask_paths\n",
    "        self._labels_count = len(mask_paths)\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        tensors = []\n",
    "\n",
    "        mask_paths = sorted(self._mask_paths)\n",
    "\n",
    "        for mask_path in mask_paths:\n",
    "            image = self._load_image_from_path(mask_path)\n",
    "            mask = self._get_mask(image)\n",
    "            tensors.append(mask)\n",
    "\n",
    "        return torch.cat(tensors, dim=0)  # (C, H, W)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_mask(image: torch.Tensor) -> torch.Tensor:\n",
    "        return image.unsqueeze(0)  # (1, H, W)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        cls_name = self.__class__.__name__\n",
    "        labels_count = self._labels_count\n",
    "        return f\"{cls_name}(labels={labels_count})\"\n",
    "\n",
    "class IndexMask(Mask):\n",
    "    def __init__(self, mask_path: str):\n",
    "        self.mask_path = mask_path\n",
    "        \n",
    "    def load(self) -> torch.Tensor:\n",
    "        return self._load_image_from_path(self.mask_path)  # (H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4532ce-8175-4302-b461-662c5a6ce719",
   "metadata": {},
   "source": [
    "# ImageIdsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9499d87e-9118-4013-88ee-8eda2112ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIdsParser:\n",
    "    @classmethod\n",
    "    def get_ids_for_dirs(cls, dirs: list[str]) -> list[str]:\n",
    "        ids = []\n",
    "        [ids.extend(cls.get_ids_for_dir(d)) for d in dirs]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def get_ids_for_dir(cls, path: str) -> list[str]:\n",
    "        ids_with_nones = [cls._get_id_from_image_path(p) for p in cls._get_items_by_path(path)]\n",
    "        ids = [i for i in ids_with_nones if i]\n",
    "        return ids\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_id_from_image_path(cls, path: str) -> str | None:\n",
    "        try:\n",
    "            return path.split(\".\")[0]  # id-with-coords.png\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_items_by_path(cls, path: str) -> list[str]:\n",
    "        try:\n",
    "            return os.listdir(path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] Path not found: {path}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "d6cfb839-da01-4649-a26f-3af1c406fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Path not found: /media/max/D09450A794509238/AgriNet-Research/agrinet/datasets/Agriculture-Vision-2021/_dataset_mini/train/aug_images/rgbn\n",
      "Found 100 image IDs.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_IDS = ImageIdsParser.get_ids_for_dirs(IMAGE_DIRS)\n",
    "print(f\"Found {len(IMAGE_IDS)} image IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c545ca-8514-4f4f-87c7-cad1a7b42bcd",
   "metadata": {},
   "source": [
    "# Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "b956e9db-0f4d-49e5-9e65-05dbfc1fb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(ABC, Dataset):\n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        self._image_ids = image_ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> ImageData:\n",
    "        image_id = self._image_ids[idx]\n",
    "        return self.get_data(image_id)\n",
    "\n",
    "    def first(self) -> ImageData | None:\n",
    "        try:\n",
    "            first_id = self._image_ids[0]\n",
    "            return self.get_data(first_id)\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576e65-0a6a-4836-ad75-c40f3ed72e55",
   "metadata": {},
   "source": [
    "# File Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "f762d464-bf74-4870-a63c-9aaeed5353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSearcher:\n",
    "    def __init__(self, search_paths: list[str]):\n",
    "        self.search_paths = search_paths\n",
    "\n",
    "    def search(self, file_name: str) -> list[str]:\n",
    "        found_paths = []\n",
    "        \n",
    "        for root_folder in self.search_paths:\n",
    "            for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "                for filename in filenames:\n",
    "                    if filename.lower() == file_name.lower():\n",
    "                        found_paths.append(os.path.join(dirpath, filename))\n",
    "        \n",
    "        return found_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea0521-ac26-4b73-a84d-7eba1a6831b3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "05407576-cf2d-4add-9a0c-8bafd4f34008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 instances\n",
      "ImageData(image_id='11IE4DKTR_11556-9586-12068-10098', image=RGBNImage, mask=OneHotMask)\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(SegmentationDataset):\n",
    "    EXPECTED_MASKS_COUNT = 9\n",
    "    \n",
    "    _image_searcher = FileSearcher(IMAGE_DIRS)\n",
    "    _mask_searcher = FileSearcher(LABEL_DIRS)\n",
    "    \n",
    "    def __init__(self, image_ids: list[str]) -> None: \n",
    "        super().__init__(image_ids=image_ids)\n",
    "    \n",
    "    def get_data(self, image_id: str) -> ImageData:\n",
    "        return ImageData(\n",
    "            image_id=image_id,\n",
    "            image=self._get_image(image_id),\n",
    "            mask=self._get_mask(image_id),\n",
    "        )\n",
    "    \n",
    "    def _get_image(self, image_id: str) -> Image:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        rgbn_paths = self._image_searcher.search(file_name)\n",
    "\n",
    "        if len(rgbn_paths) == 0 or len(rgbn_paths) > 1:\n",
    "            raise Exception(f\"Expected exactly one RGBN image for ID '{image_id}', found {len(rgbn_paths)}.\")\n",
    "\n",
    "        return RGBNImage(rgbn_paths[0])\n",
    "\n",
    "    def _get_mask(self, image_id: str) -> Mask:\n",
    "        file_name = self._get_file_name(image_id)\n",
    "        masks = self._mask_searcher.search(file_name)\n",
    "\n",
    "        if not masks:\n",
    "            raise Exception(f\"No masks found for image ID '{image_id}'.\")\n",
    "\n",
    "        self._validate_masks(masks)\n",
    "\n",
    "        return OneHotMask(*masks)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_masks(cls, masks: list[str]) -> None:\n",
    "        found = len(masks)\n",
    "        expected = cls.EXPECTED_MASKS_COUNT\n",
    "\n",
    "        if found != expected:\n",
    "            raise Exception(\"Failed parse masks, {expected=}, {found=}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(image_id: str) -> str:\n",
    "        return f\"{image_id}.png\"\n",
    "\n",
    "\n",
    "def _test() -> None:\n",
    "    train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "    image_data = train_dataset.first()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} instances\")\n",
    "    \n",
    "    if image_data:\n",
    "        print(image_data)\n",
    "\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef1f41-5b1c-4f15-9e9e-e6718576200c",
   "metadata": {},
   "source": [
    "# Build model with SegmentationModels.Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a9b96e7e-1462-4225-8ca7-40da70ab1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.metrics import iou_score, get_stats\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "fd2aee77-3ed6-4102-b879-0f102daa4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGBN_CHANNELS = 4\n",
    "CLASSES_COUNT = 9\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=RGBN_CHANNELS,\n",
    "    classes=CLASSES_COUNT,\n",
    "    decoder_attention_type=\"scse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbe565-c5f2-4cb7-84e3-8bf4b9850385",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c5f1aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_workers() -> int:\n",
    "    num_workers = os.cpu_count() - 1\n",
    "\n",
    "    if num_workers is None or num_workers < 1:\n",
    "        num_workers = 1\n",
    "\n",
    "    return num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "57827020-2595-47f3-aed4-18666065d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "train_dataset = TrainDataset(image_ids=IMAGE_IDS)\n",
    "\n",
    "# TODO parse val and test\n",
    "val_dataset = train_dataset\n",
    "test_dataset = train_dataset\n",
    "\n",
    "NUM_WORKERS = get_num_workers()\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de372f-82a4-41e0-8a49-4e2287254c3c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a61900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    return dice_loss(pred, target)\n",
    "\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f9511591-fde0-4caf-a69a-f64daa653f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs: torch.Tensor, masks: torch.Tensor, num_classes=9):\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    tp, fp, fn, tn = get_stats(preds, masks, mode='multiclass', num_classes=num_classes)\n",
    "\n",
    "    iou = iou_score(tp, fp, fn, tn, reduction='micro')\n",
    "    dice = (2 * tp.sum()) / (2 * tp.sum() + fp.sum() + fn.sum())\n",
    "\n",
    "    return dice.item(), iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "437c9d2f-4bee-46ec-bcb7-edac04d2e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler=None\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with autocast(device_type):\n",
    "                outputs = model(imgs)\n",
    "                loss = calculate_loss(outputs, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate_one_epoch(model: torch.nn.Module, loader: DataLoader, num_classes=9) -> tuple[float, float, float]:\n",
    "    \"\"\"Validate the model for one epoch. Returns loss, dice, iou.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = calculate_loss(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            \n",
    "            dice, iou = calculate_metrics(outputs, masks, num_classes=num_classes)\n",
    "            total_dice += dice * imgs.size(0)\n",
    "            total_iou += iou * imgs.size(0)\n",
    "    \n",
    "    dataset_size = len(loader.dataset)\n",
    "    return (total_loss / dataset_size,\n",
    "            total_dice / dataset_size,\n",
    "            total_iou / dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2b9c9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_type() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "0ba7cba6-6324-4cb8-b14e-775818523aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device_type = get_device_type()\n",
    "device = torch.device(device_type)\n",
    "print(f\"Using device: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "4ac26037",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler(device_type) if device_type == 'cuda' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015ff3a-34d6-4944-bab6-128720e9d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "    val_loss, val_dice, val_iou = validate_one_epoch(model, val_loader, num_classes=9)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"| Train Loss: {train_loss:.4f} \"\n",
    "          f\"| Val Loss: {val_loss:.4f} \"\n",
    "          f\"| Dice: {val_dice:.4f} \"\n",
    "          f\"| IoU: {val_iou:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"unet_epoch_{epoch+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
